{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Выгрузка данных**"
      ],
      "metadata": {
        "id": "WdnddBMTjY6V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4nAOSutX7czK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73962c27-1041-4bfb-c909-ee6d371a89c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/eeg_aw_valence_effort.csv')"
      ],
      "metadata": {
        "id": "BXJ4HI3v8nqq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Обработка данных**"
      ],
      "metadata": {
        "id": "AkJZLUMpjf18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(df.columns[[0, 1, 2, 3]], axis=1)\n",
        "df.dropna(inplace=True)\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "rh9Y2PfD8M0u",
        "outputId": "01f91d3b-7df1-4e7c-e076-98d662426495"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      buy    val12    val15   effort\n",
              "0       0  0.05437 -0.81024  0.14033\n",
              "1       0  0.18803  1.10880  0.05269\n",
              "2       0  0.06618 -0.14021  0.52483\n",
              "3       1  0.89158 -0.89813  0.22057\n",
              "4       0  0.47358 -0.08872  0.01328\n",
              "...   ...      ...      ...      ...\n",
              "5897    1 -0.08974  0.30855 -0.32381\n",
              "5898    0 -0.45575 -0.04902 -0.20760\n",
              "5899    0 -0.15555  0.25523  0.21105\n",
              "5900    0  0.70788 -0.05353  0.52069\n",
              "5901    0  0.15303 -0.22811  0.43889\n",
              "\n",
              "[5606 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2959d7ff-4f8b-4d5f-a0d0-e185c8aab7c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>buy</th>\n",
              "      <th>val12</th>\n",
              "      <th>val15</th>\n",
              "      <th>effort</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.05437</td>\n",
              "      <td>-0.81024</td>\n",
              "      <td>0.14033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.18803</td>\n",
              "      <td>1.10880</td>\n",
              "      <td>0.05269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.06618</td>\n",
              "      <td>-0.14021</td>\n",
              "      <td>0.52483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.89158</td>\n",
              "      <td>-0.89813</td>\n",
              "      <td>0.22057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.47358</td>\n",
              "      <td>-0.08872</td>\n",
              "      <td>0.01328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5897</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.08974</td>\n",
              "      <td>0.30855</td>\n",
              "      <td>-0.32381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5898</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.45575</td>\n",
              "      <td>-0.04902</td>\n",
              "      <td>-0.20760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5899</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.15555</td>\n",
              "      <td>0.25523</td>\n",
              "      <td>0.21105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5900</th>\n",
              "      <td>0</td>\n",
              "      <td>0.70788</td>\n",
              "      <td>-0.05353</td>\n",
              "      <td>0.52069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5901</th>\n",
              "      <td>0</td>\n",
              "      <td>0.15303</td>\n",
              "      <td>-0.22811</td>\n",
              "      <td>0.43889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5606 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2959d7ff-4f8b-4d5f-a0d0-e185c8aab7c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2959d7ff-4f8b-4d5f-a0d0-e185c8aab7c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2959d7ff-4f8b-4d5f-a0d0-e185c8aab7c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bdc23a1d-13d9-43fc-be0f-f5336f8811f0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bdc23a1d-13d9-43fc-be0f-f5336f8811f0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bdc23a1d-13d9-43fc-be0f-f5336f8811f0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_33c295ca-d8f9-4ac1-bbd1-616742e65217\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_33c295ca-d8f9-4ac1-bbd1-616742e65217 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5606,\n  \"fields\": [\n    {\n      \"column\": \"buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val12\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2401976127182235,\n        \"min\": -4.38012,\n        \"max\": 25.51997,\n        \"num_unique_values\": 5531,\n        \"samples\": [\n          1.33618,\n          -0.1555\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val15\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3445302849742764,\n        \"min\": -121.9844,\n        \"max\": 38.05112,\n        \"num_unique_values\": 5575,\n        \"samples\": [\n          -1.87086,\n          4.08274\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"effort\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3481950830715452,\n        \"min\": -0.99764,\n        \"max\": 0.99989,\n        \"num_unique_values\": 5476,\n        \"samples\": [\n          0.08232,\n          -0.26954\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Визуализация данных**"
      ],
      "metadata": {
        "id": "aJblLtHgjm9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "y = df['buy'].values.astype(np.float32)\n",
        "count_zeros = (y == 0).sum()\n",
        "count_ones = (y == 1).sum()\n",
        "\n",
        "plt.bar([0, 1], [count_zeros, count_ones])\n",
        "plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Classes in data')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "mrMmHfiaPVA_",
        "outputId": "ac95e90f-56c0-4bf4-e94d-54fb2bc7b38f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **устранение дисбаланса путем дублирования данных**"
      ],
      "metadata": {
        "id": "PLfvzGgMjtVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_class_1 = df['buy'].sum()\n",
        "\n",
        "class_1_data = df[df['buy'] == 1]\n",
        "\n",
        "while len(class_1_data) < len(df[df['buy'] == 0]):\n",
        "    class_1_data = pd.concat([class_1_data, class_1_data.head(len(df[df['buy'] == 0]) - len(class_1_data))])\n",
        "\n",
        "balanced_df = pd.concat([df[df['buy'] == 0], class_1_data])\n",
        "\n",
        "\n",
        "y = balanced_df['buy'].values.astype(np.float32)\n",
        "count_zeros = (y == 0).sum()\n",
        "count_ones = (y == 1).sum()\n",
        "\n",
        "\n",
        "plt.bar([0, 1], [count_zeros, count_ones])\n",
        "plt.xticks([0, 1], ['Class 0', 'Class 1'])\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Classes in data')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "_F_L2KI5QB7o",
        "outputId": "19a3f626-10e2-4fcc-faa9-184dbbc60c68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Обучение модели**"
      ],
      "metadata": {
        "id": "YKrsP2wqj6sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "\n",
        "X = balanced_df[['val12', 'val15', 'effort']].values.astype(np.float32)\n",
        "y = balanced_df['buy'].values.astype(np.float32)\n",
        "\n",
        "\n",
        "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_rest, y_rest, test_size=0.5, random_state=42)\n",
        "\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32, device=device).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32, device=device).view(-1, 1)\n",
        "\n",
        "X_valid_tensor = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
        "y_valid_tensor = torch.tensor(y_val, dtype=torch.float32, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(DNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 300)\n",
        "        self.fc2 = nn.Linear(300, 200)\n",
        "        self.fc3 = nn.Linear(200, 100)\n",
        "        self.fc4 = nn.Linear(100, 50)\n",
        "        self.fc5 = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.sigmoid(self.fc5(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "model = DNN(input_size).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "num_epochs = 1000\n",
        "best_loss = float('inf')\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(X_valid_tensor)\n",
        "        val_loss = criterion(val_outputs, y_valid_tensor)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Valid Loss: {val_loss.item():.4f}')\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_epoch = epoch+1\n",
        "        torch.save(model.state_dict(), f'best_model.pt')\n",
        "\n",
        "\n",
        "print(f'best model at epoch {best_epoch}')\n",
        "final_model = DNN(input_size).to(device)\n",
        "final_model.load_state_dict(torch.load('best_model.pt'))\n",
        "final_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = final_model(X_test_tensor)\n",
        "    predicted = (outputs > 0.5).float()\n",
        "    accuracy = (predicted == y_test_tensor).float().mean()\n",
        "    print(f'Accuracy on test set: {accuracy.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRyD2_LW-hNK",
        "outputId": "c41052f0-39c7-4a2b-8889-303501290c75"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n",
            "Epoch [1/1000], Train Loss: 0.6937, Valid Loss: 0.6840\n",
            "Epoch [2/1000], Train Loss: 0.6882, Valid Loss: 0.6955\n",
            "Epoch [3/1000], Train Loss: 0.6906, Valid Loss: 0.7125\n",
            "Epoch [4/1000], Train Loss: 0.7272, Valid Loss: 0.6725\n",
            "Epoch [5/1000], Train Loss: 0.6756, Valid Loss: 0.6905\n",
            "Epoch [6/1000], Train Loss: 0.6864, Valid Loss: 0.6848\n",
            "Epoch [7/1000], Train Loss: 0.6802, Valid Loss: 0.6781\n",
            "Epoch [8/1000], Train Loss: 0.6746, Valid Loss: 0.6730\n",
            "Epoch [9/1000], Train Loss: 0.6704, Valid Loss: 0.6678\n",
            "Epoch [10/1000], Train Loss: 0.6652, Valid Loss: 0.6637\n",
            "Epoch [11/1000], Train Loss: 0.6608, Valid Loss: 0.6634\n",
            "Epoch [12/1000], Train Loss: 0.6609, Valid Loss: 0.6628\n",
            "Epoch [13/1000], Train Loss: 0.6594, Valid Loss: 0.6626\n",
            "Epoch [14/1000], Train Loss: 0.6564, Valid Loss: 0.6615\n",
            "Epoch [15/1000], Train Loss: 0.6543, Valid Loss: 0.6617\n",
            "Epoch [16/1000], Train Loss: 0.6532, Valid Loss: 0.6612\n",
            "Epoch [17/1000], Train Loss: 0.6525, Valid Loss: 0.6599\n",
            "Epoch [18/1000], Train Loss: 0.6519, Valid Loss: 0.6611\n",
            "Epoch [19/1000], Train Loss: 0.6513, Valid Loss: 0.6582\n",
            "Epoch [20/1000], Train Loss: 0.6512, Valid Loss: 0.6588\n",
            "Epoch [21/1000], Train Loss: 0.6491, Valid Loss: 0.6565\n",
            "Epoch [22/1000], Train Loss: 0.6478, Valid Loss: 0.6543\n",
            "Epoch [23/1000], Train Loss: 0.6474, Valid Loss: 0.6540\n",
            "Epoch [24/1000], Train Loss: 0.6459, Valid Loss: 0.6534\n",
            "Epoch [25/1000], Train Loss: 0.6452, Valid Loss: 0.6514\n",
            "Epoch [26/1000], Train Loss: 0.6447, Valid Loss: 0.6515\n",
            "Epoch [27/1000], Train Loss: 0.6432, Valid Loss: 0.6520\n",
            "Epoch [28/1000], Train Loss: 0.6423, Valid Loss: 0.6496\n",
            "Epoch [29/1000], Train Loss: 0.6412, Valid Loss: 0.6500\n",
            "Epoch [30/1000], Train Loss: 0.6396, Valid Loss: 0.6510\n",
            "Epoch [31/1000], Train Loss: 0.6384, Valid Loss: 0.6481\n",
            "Epoch [32/1000], Train Loss: 0.6374, Valid Loss: 0.6488\n",
            "Epoch [33/1000], Train Loss: 0.6351, Valid Loss: 0.6466\n",
            "Epoch [34/1000], Train Loss: 0.6334, Valid Loss: 0.6446\n",
            "Epoch [35/1000], Train Loss: 0.6317, Valid Loss: 0.6469\n",
            "Epoch [36/1000], Train Loss: 0.6308, Valid Loss: 0.6432\n",
            "Epoch [37/1000], Train Loss: 0.6308, Valid Loss: 0.6489\n",
            "Epoch [38/1000], Train Loss: 0.6295, Valid Loss: 0.6423\n",
            "Epoch [39/1000], Train Loss: 0.6263, Valid Loss: 0.6418\n",
            "Epoch [40/1000], Train Loss: 0.6248, Valid Loss: 0.6464\n",
            "Epoch [41/1000], Train Loss: 0.6247, Valid Loss: 0.6431\n",
            "Epoch [42/1000], Train Loss: 0.6241, Valid Loss: 0.6441\n",
            "Epoch [43/1000], Train Loss: 0.6237, Valid Loss: 0.6411\n",
            "Epoch [44/1000], Train Loss: 0.6207, Valid Loss: 0.6449\n",
            "Epoch [45/1000], Train Loss: 0.6217, Valid Loss: 0.6494\n",
            "Epoch [46/1000], Train Loss: 0.6232, Valid Loss: 0.6419\n",
            "Epoch [47/1000], Train Loss: 0.6222, Valid Loss: 0.6397\n",
            "Epoch [48/1000], Train Loss: 0.6175, Valid Loss: 0.6492\n",
            "Epoch [49/1000], Train Loss: 0.6202, Valid Loss: 0.6445\n",
            "Epoch [50/1000], Train Loss: 0.6160, Valid Loss: 0.6397\n",
            "Epoch [51/1000], Train Loss: 0.6157, Valid Loss: 0.6403\n",
            "Epoch [52/1000], Train Loss: 0.6146, Valid Loss: 0.6435\n",
            "Epoch [53/1000], Train Loss: 0.6121, Valid Loss: 0.6457\n",
            "Epoch [54/1000], Train Loss: 0.6129, Valid Loss: 0.6392\n",
            "Epoch [55/1000], Train Loss: 0.6109, Valid Loss: 0.6385\n",
            "Epoch [56/1000], Train Loss: 0.6121, Valid Loss: 0.6393\n",
            "Epoch [57/1000], Train Loss: 0.6079, Valid Loss: 0.6438\n",
            "Epoch [58/1000], Train Loss: 0.6081, Valid Loss: 0.6397\n",
            "Epoch [59/1000], Train Loss: 0.6058, Valid Loss: 0.6379\n",
            "Epoch [60/1000], Train Loss: 0.6063, Valid Loss: 0.6396\n",
            "Epoch [61/1000], Train Loss: 0.6041, Valid Loss: 0.6438\n",
            "Epoch [62/1000], Train Loss: 0.6042, Valid Loss: 0.6382\n",
            "Epoch [63/1000], Train Loss: 0.6046, Valid Loss: 0.6393\n",
            "Epoch [64/1000], Train Loss: 0.6077, Valid Loss: 0.6357\n",
            "Epoch [65/1000], Train Loss: 0.6000, Valid Loss: 0.6452\n",
            "Epoch [66/1000], Train Loss: 0.6014, Valid Loss: 0.6433\n",
            "Epoch [67/1000], Train Loss: 0.6046, Valid Loss: 0.6418\n",
            "Epoch [68/1000], Train Loss: 0.6104, Valid Loss: 0.6323\n",
            "Epoch [69/1000], Train Loss: 0.5984, Valid Loss: 0.6477\n",
            "Epoch [70/1000], Train Loss: 0.6018, Valid Loss: 0.6510\n",
            "Epoch [71/1000], Train Loss: 0.6040, Valid Loss: 0.6352\n",
            "Epoch [72/1000], Train Loss: 0.5996, Valid Loss: 0.6329\n",
            "Epoch [73/1000], Train Loss: 0.5982, Valid Loss: 0.6394\n",
            "Epoch [74/1000], Train Loss: 0.5951, Valid Loss: 0.6427\n",
            "Epoch [75/1000], Train Loss: 0.5924, Valid Loss: 0.6334\n",
            "Epoch [76/1000], Train Loss: 0.5896, Valid Loss: 0.6294\n",
            "Epoch [77/1000], Train Loss: 0.5920, Valid Loss: 0.6309\n",
            "Epoch [78/1000], Train Loss: 0.5849, Valid Loss: 0.6460\n",
            "Epoch [79/1000], Train Loss: 0.5896, Valid Loss: 0.6332\n",
            "Epoch [80/1000], Train Loss: 0.5851, Valid Loss: 0.6296\n",
            "Epoch [81/1000], Train Loss: 0.5909, Valid Loss: 0.6274\n",
            "Epoch [82/1000], Train Loss: 0.5793, Valid Loss: 0.6464\n",
            "Epoch [83/1000], Train Loss: 0.5870, Valid Loss: 0.6444\n",
            "Epoch [84/1000], Train Loss: 0.5953, Valid Loss: 0.6331\n",
            "Epoch [85/1000], Train Loss: 0.5967, Valid Loss: 0.6287\n",
            "Epoch [86/1000], Train Loss: 0.5771, Valid Loss: 0.6530\n",
            "Epoch [87/1000], Train Loss: 0.5926, Valid Loss: 0.6298\n",
            "Epoch [88/1000], Train Loss: 0.5826, Valid Loss: 0.6223\n",
            "Epoch [89/1000], Train Loss: 0.5870, Valid Loss: 0.6246\n",
            "Epoch [90/1000], Train Loss: 0.5790, Valid Loss: 0.6474\n",
            "Epoch [91/1000], Train Loss: 0.5835, Valid Loss: 0.6346\n",
            "Epoch [92/1000], Train Loss: 0.5753, Valid Loss: 0.6202\n",
            "Epoch [93/1000], Train Loss: 0.5764, Valid Loss: 0.6185\n",
            "Epoch [94/1000], Train Loss: 0.5742, Valid Loss: 0.6301\n",
            "Epoch [95/1000], Train Loss: 0.5713, Valid Loss: 0.6328\n",
            "Epoch [96/1000], Train Loss: 0.5685, Valid Loss: 0.6209\n",
            "Epoch [97/1000], Train Loss: 0.5668, Valid Loss: 0.6152\n",
            "Epoch [98/1000], Train Loss: 0.5648, Valid Loss: 0.6227\n",
            "Epoch [99/1000], Train Loss: 0.5638, Valid Loss: 0.6219\n",
            "Epoch [100/1000], Train Loss: 0.5632, Valid Loss: 0.6185\n",
            "Epoch [101/1000], Train Loss: 0.5654, Valid Loss: 0.6260\n",
            "Epoch [102/1000], Train Loss: 0.5712, Valid Loss: 0.6243\n",
            "Epoch [103/1000], Train Loss: 0.5698, Valid Loss: 0.6335\n",
            "Epoch [104/1000], Train Loss: 0.5763, Valid Loss: 0.6237\n",
            "Epoch [105/1000], Train Loss: 0.5674, Valid Loss: 0.6191\n",
            "Epoch [106/1000], Train Loss: 0.5645, Valid Loss: 0.6201\n",
            "Epoch [107/1000], Train Loss: 0.5611, Valid Loss: 0.6199\n",
            "Epoch [108/1000], Train Loss: 0.5585, Valid Loss: 0.6198\n",
            "Epoch [109/1000], Train Loss: 0.5571, Valid Loss: 0.6239\n",
            "Epoch [110/1000], Train Loss: 0.5540, Valid Loss: 0.6297\n",
            "Epoch [111/1000], Train Loss: 0.5570, Valid Loss: 0.6127\n",
            "Epoch [112/1000], Train Loss: 0.5525, Valid Loss: 0.6156\n",
            "Epoch [113/1000], Train Loss: 0.5546, Valid Loss: 0.6186\n",
            "Epoch [114/1000], Train Loss: 0.5476, Valid Loss: 0.6151\n",
            "Epoch [115/1000], Train Loss: 0.5528, Valid Loss: 0.6160\n",
            "Epoch [116/1000], Train Loss: 0.5510, Valid Loss: 0.6308\n",
            "Epoch [117/1000], Train Loss: 0.5600, Valid Loss: 0.6163\n",
            "Epoch [118/1000], Train Loss: 0.5547, Valid Loss: 0.6101\n",
            "Epoch [119/1000], Train Loss: 0.5440, Valid Loss: 0.6255\n",
            "Epoch [120/1000], Train Loss: 0.5416, Valid Loss: 0.6132\n",
            "Epoch [121/1000], Train Loss: 0.5411, Valid Loss: 0.6059\n",
            "Epoch [122/1000], Train Loss: 0.5453, Valid Loss: 0.6207\n",
            "Epoch [123/1000], Train Loss: 0.5488, Valid Loss: 0.6096\n",
            "Epoch [124/1000], Train Loss: 0.5429, Valid Loss: 0.5948\n",
            "Epoch [125/1000], Train Loss: 0.5291, Valid Loss: 0.6122\n",
            "Epoch [126/1000], Train Loss: 0.5323, Valid Loss: 0.6046\n",
            "Epoch [127/1000], Train Loss: 0.5277, Valid Loss: 0.5941\n",
            "Epoch [128/1000], Train Loss: 0.5278, Valid Loss: 0.6012\n",
            "Epoch [129/1000], Train Loss: 0.5251, Valid Loss: 0.5965\n",
            "Epoch [130/1000], Train Loss: 0.5193, Valid Loss: 0.5962\n",
            "Epoch [131/1000], Train Loss: 0.5197, Valid Loss: 0.6042\n",
            "Epoch [132/1000], Train Loss: 0.5218, Valid Loss: 0.6132\n",
            "Epoch [133/1000], Train Loss: 0.5353, Valid Loss: 0.6667\n",
            "Epoch [134/1000], Train Loss: 0.5957, Valid Loss: 0.6195\n",
            "Epoch [135/1000], Train Loss: 0.5648, Valid Loss: 0.6081\n",
            "Epoch [136/1000], Train Loss: 0.5490, Valid Loss: 0.6255\n",
            "Epoch [137/1000], Train Loss: 0.5599, Valid Loss: 0.6035\n",
            "Epoch [138/1000], Train Loss: 0.5348, Valid Loss: 0.6112\n",
            "Epoch [139/1000], Train Loss: 0.5465, Valid Loss: 0.6184\n",
            "Epoch [140/1000], Train Loss: 0.5396, Valid Loss: 0.6111\n",
            "Epoch [141/1000], Train Loss: 0.5308, Valid Loss: 0.6182\n",
            "Epoch [142/1000], Train Loss: 0.5403, Valid Loss: 0.5893\n",
            "Epoch [143/1000], Train Loss: 0.5290, Valid Loss: 0.5984\n",
            "Epoch [144/1000], Train Loss: 0.5279, Valid Loss: 0.6031\n",
            "Epoch [145/1000], Train Loss: 0.5267, Valid Loss: 0.5831\n",
            "Epoch [146/1000], Train Loss: 0.5176, Valid Loss: 0.5882\n",
            "Epoch [147/1000], Train Loss: 0.5197, Valid Loss: 0.5935\n",
            "Epoch [148/1000], Train Loss: 0.5110, Valid Loss: 0.5942\n",
            "Epoch [149/1000], Train Loss: 0.5114, Valid Loss: 0.5773\n",
            "Epoch [150/1000], Train Loss: 0.5074, Valid Loss: 0.5810\n",
            "Epoch [151/1000], Train Loss: 0.5081, Valid Loss: 0.5815\n",
            "Epoch [152/1000], Train Loss: 0.5007, Valid Loss: 0.5817\n",
            "Epoch [153/1000], Train Loss: 0.5026, Valid Loss: 0.5731\n",
            "Epoch [154/1000], Train Loss: 0.4953, Valid Loss: 0.5816\n",
            "Epoch [155/1000], Train Loss: 0.4964, Valid Loss: 0.5776\n",
            "Epoch [156/1000], Train Loss: 0.4943, Valid Loss: 0.5798\n",
            "Epoch [157/1000], Train Loss: 0.4995, Valid Loss: 0.5951\n",
            "Epoch [158/1000], Train Loss: 0.5054, Valid Loss: 0.5809\n",
            "Epoch [159/1000], Train Loss: 0.5073, Valid Loss: 0.5943\n",
            "Epoch [160/1000], Train Loss: 0.5066, Valid Loss: 0.5906\n",
            "Epoch [161/1000], Train Loss: 0.5022, Valid Loss: 0.5670\n",
            "Epoch [162/1000], Train Loss: 0.4959, Valid Loss: 0.5826\n",
            "Epoch [163/1000], Train Loss: 0.4952, Valid Loss: 0.5751\n",
            "Epoch [164/1000], Train Loss: 0.4868, Valid Loss: 0.5726\n",
            "Epoch [165/1000], Train Loss: 0.4880, Valid Loss: 0.5790\n",
            "Epoch [166/1000], Train Loss: 0.4840, Valid Loss: 0.5728\n",
            "Epoch [167/1000], Train Loss: 0.4884, Valid Loss: 0.5753\n",
            "Epoch [168/1000], Train Loss: 0.4856, Valid Loss: 0.5679\n",
            "Epoch [169/1000], Train Loss: 0.4780, Valid Loss: 0.5662\n",
            "Epoch [170/1000], Train Loss: 0.4776, Valid Loss: 0.5572\n",
            "Epoch [171/1000], Train Loss: 0.4708, Valid Loss: 0.5656\n",
            "Epoch [172/1000], Train Loss: 0.4712, Valid Loss: 0.5529\n",
            "Epoch [173/1000], Train Loss: 0.4699, Valid Loss: 0.5715\n",
            "Epoch [174/1000], Train Loss: 0.4729, Valid Loss: 0.5967\n",
            "Epoch [175/1000], Train Loss: 0.5220, Valid Loss: 0.8477\n",
            "Epoch [176/1000], Train Loss: 0.7413, Valid Loss: 0.6374\n",
            "Epoch [177/1000], Train Loss: 0.6073, Valid Loss: 0.6610\n",
            "Epoch [178/1000], Train Loss: 0.6017, Valid Loss: 0.6340\n",
            "Epoch [179/1000], Train Loss: 0.5505, Valid Loss: 0.6886\n",
            "Epoch [180/1000], Train Loss: 0.6068, Valid Loss: 0.6245\n",
            "Epoch [181/1000], Train Loss: 0.5544, Valid Loss: 0.6250\n",
            "Epoch [182/1000], Train Loss: 0.5690, Valid Loss: 0.5997\n",
            "Epoch [183/1000], Train Loss: 0.5470, Valid Loss: 0.5976\n",
            "Epoch [184/1000], Train Loss: 0.5410, Valid Loss: 0.6114\n",
            "Epoch [185/1000], Train Loss: 0.5467, Valid Loss: 0.6208\n",
            "Epoch [186/1000], Train Loss: 0.5444, Valid Loss: 0.6072\n",
            "Epoch [187/1000], Train Loss: 0.5329, Valid Loss: 0.5968\n",
            "Epoch [188/1000], Train Loss: 0.5318, Valid Loss: 0.5935\n",
            "Epoch [189/1000], Train Loss: 0.5314, Valid Loss: 0.5918\n",
            "Epoch [190/1000], Train Loss: 0.5275, Valid Loss: 0.5917\n",
            "Epoch [191/1000], Train Loss: 0.5205, Valid Loss: 0.5895\n",
            "Epoch [192/1000], Train Loss: 0.5133, Valid Loss: 0.5843\n",
            "Epoch [193/1000], Train Loss: 0.5086, Valid Loss: 0.5874\n",
            "Epoch [194/1000], Train Loss: 0.5086, Valid Loss: 0.5833\n",
            "Epoch [195/1000], Train Loss: 0.5011, Valid Loss: 0.5844\n",
            "Epoch [196/1000], Train Loss: 0.4973, Valid Loss: 0.5843\n",
            "Epoch [197/1000], Train Loss: 0.4953, Valid Loss: 0.5742\n",
            "Epoch [198/1000], Train Loss: 0.4912, Valid Loss: 0.5705\n",
            "Epoch [199/1000], Train Loss: 0.4878, Valid Loss: 0.5736\n",
            "Epoch [200/1000], Train Loss: 0.4856, Valid Loss: 0.5697\n",
            "Epoch [201/1000], Train Loss: 0.4819, Valid Loss: 0.5681\n",
            "Epoch [202/1000], Train Loss: 0.4803, Valid Loss: 0.5674\n",
            "Epoch [203/1000], Train Loss: 0.4770, Valid Loss: 0.5664\n",
            "Epoch [204/1000], Train Loss: 0.4741, Valid Loss: 0.5698\n",
            "Epoch [205/1000], Train Loss: 0.4716, Valid Loss: 0.5617\n",
            "Epoch [206/1000], Train Loss: 0.4671, Valid Loss: 0.5550\n",
            "Epoch [207/1000], Train Loss: 0.4660, Valid Loss: 0.5532\n",
            "Epoch [208/1000], Train Loss: 0.4626, Valid Loss: 0.5573\n",
            "Epoch [209/1000], Train Loss: 0.4597, Valid Loss: 0.5570\n",
            "Epoch [210/1000], Train Loss: 0.4574, Valid Loss: 0.5511\n",
            "Epoch [211/1000], Train Loss: 0.4545, Valid Loss: 0.5477\n",
            "Epoch [212/1000], Train Loss: 0.4517, Valid Loss: 0.5446\n",
            "Epoch [213/1000], Train Loss: 0.4489, Valid Loss: 0.5443\n",
            "Epoch [214/1000], Train Loss: 0.4476, Valid Loss: 0.5443\n",
            "Epoch [215/1000], Train Loss: 0.4459, Valid Loss: 0.5437\n",
            "Epoch [216/1000], Train Loss: 0.4453, Valid Loss: 0.5459\n",
            "Epoch [217/1000], Train Loss: 0.4482, Valid Loss: 0.5521\n",
            "Epoch [218/1000], Train Loss: 0.4448, Valid Loss: 0.5501\n",
            "Epoch [219/1000], Train Loss: 0.4467, Valid Loss: 0.5438\n",
            "Epoch [220/1000], Train Loss: 0.4416, Valid Loss: 0.5518\n",
            "Epoch [221/1000], Train Loss: 0.4436, Valid Loss: 0.5378\n",
            "Epoch [222/1000], Train Loss: 0.4331, Valid Loss: 0.5388\n",
            "Epoch [223/1000], Train Loss: 0.4332, Valid Loss: 0.5289\n",
            "Epoch [224/1000], Train Loss: 0.4280, Valid Loss: 0.5301\n",
            "Epoch [225/1000], Train Loss: 0.4250, Valid Loss: 0.5317\n",
            "Epoch [226/1000], Train Loss: 0.4229, Valid Loss: 0.5328\n",
            "Epoch [227/1000], Train Loss: 0.4248, Valid Loss: 0.5308\n",
            "Epoch [228/1000], Train Loss: 0.4267, Valid Loss: 0.5796\n",
            "Epoch [229/1000], Train Loss: 0.4615, Valid Loss: 0.5483\n",
            "Epoch [230/1000], Train Loss: 0.4544, Valid Loss: 0.5791\n",
            "Epoch [231/1000], Train Loss: 0.4753, Valid Loss: 0.5465\n",
            "Epoch [232/1000], Train Loss: 0.4418, Valid Loss: 0.5693\n",
            "Epoch [233/1000], Train Loss: 0.4479, Valid Loss: 0.5723\n",
            "Epoch [234/1000], Train Loss: 0.4779, Valid Loss: 0.5280\n",
            "Epoch [235/1000], Train Loss: 0.4437, Valid Loss: 0.5679\n",
            "Epoch [236/1000], Train Loss: 0.4547, Valid Loss: 0.5697\n",
            "Epoch [237/1000], Train Loss: 0.4525, Valid Loss: 0.5664\n",
            "Epoch [238/1000], Train Loss: 0.4673, Valid Loss: 0.5955\n",
            "Epoch [239/1000], Train Loss: 0.4559, Valid Loss: 0.5302\n",
            "Epoch [240/1000], Train Loss: 0.4406, Valid Loss: 0.5280\n",
            "Epoch [241/1000], Train Loss: 0.4388, Valid Loss: 0.5629\n",
            "Epoch [242/1000], Train Loss: 0.4435, Valid Loss: 0.5733\n",
            "Epoch [243/1000], Train Loss: 0.4557, Valid Loss: 0.5682\n",
            "Epoch [244/1000], Train Loss: 0.4458, Valid Loss: 0.5484\n",
            "Epoch [245/1000], Train Loss: 0.4340, Valid Loss: 0.5553\n",
            "Epoch [246/1000], Train Loss: 0.4430, Valid Loss: 0.5360\n",
            "Epoch [247/1000], Train Loss: 0.4269, Valid Loss: 0.5270\n",
            "Epoch [248/1000], Train Loss: 0.4354, Valid Loss: 0.5437\n",
            "Epoch [249/1000], Train Loss: 0.4246, Valid Loss: 0.5326\n",
            "Epoch [250/1000], Train Loss: 0.4214, Valid Loss: 0.5246\n",
            "Epoch [251/1000], Train Loss: 0.4190, Valid Loss: 0.5351\n",
            "Epoch [252/1000], Train Loss: 0.4174, Valid Loss: 0.5150\n",
            "Epoch [253/1000], Train Loss: 0.4110, Valid Loss: 0.5273\n",
            "Epoch [254/1000], Train Loss: 0.4115, Valid Loss: 0.5155\n",
            "Epoch [255/1000], Train Loss: 0.4039, Valid Loss: 0.5141\n",
            "Epoch [256/1000], Train Loss: 0.4018, Valid Loss: 0.5199\n",
            "Epoch [257/1000], Train Loss: 0.4003, Valid Loss: 0.5018\n",
            "Epoch [258/1000], Train Loss: 0.3996, Valid Loss: 0.5318\n",
            "Epoch [259/1000], Train Loss: 0.4035, Valid Loss: 0.5024\n",
            "Epoch [260/1000], Train Loss: 0.4016, Valid Loss: 0.5547\n",
            "Epoch [261/1000], Train Loss: 0.4144, Valid Loss: 0.5214\n",
            "Epoch [262/1000], Train Loss: 0.4196, Valid Loss: 0.5996\n",
            "Epoch [263/1000], Train Loss: 0.4498, Valid Loss: 0.5045\n",
            "Epoch [264/1000], Train Loss: 0.3999, Valid Loss: 0.5052\n",
            "Epoch [265/1000], Train Loss: 0.3907, Valid Loss: 0.5747\n",
            "Epoch [266/1000], Train Loss: 0.4327, Valid Loss: 0.5165\n",
            "Epoch [267/1000], Train Loss: 0.4082, Valid Loss: 0.5130\n",
            "Epoch [268/1000], Train Loss: 0.3859, Valid Loss: 0.5145\n",
            "Epoch [269/1000], Train Loss: 0.3947, Valid Loss: 0.4967\n",
            "Epoch [270/1000], Train Loss: 0.3946, Valid Loss: 0.5396\n",
            "Epoch [271/1000], Train Loss: 0.3998, Valid Loss: 0.5059\n",
            "Epoch [272/1000], Train Loss: 0.3847, Valid Loss: 0.4945\n",
            "Epoch [273/1000], Train Loss: 0.3771, Valid Loss: 0.5209\n",
            "Epoch [274/1000], Train Loss: 0.3806, Valid Loss: 0.5003\n",
            "Epoch [275/1000], Train Loss: 0.3827, Valid Loss: 0.5212\n",
            "Epoch [276/1000], Train Loss: 0.3955, Valid Loss: 0.4995\n",
            "Epoch [277/1000], Train Loss: 0.3880, Valid Loss: 0.5293\n",
            "Epoch [278/1000], Train Loss: 0.3833, Valid Loss: 0.5075\n",
            "Epoch [279/1000], Train Loss: 0.3760, Valid Loss: 0.5147\n",
            "Epoch [280/1000], Train Loss: 0.3801, Valid Loss: 0.4949\n",
            "Epoch [281/1000], Train Loss: 0.3694, Valid Loss: 0.5063\n",
            "Epoch [282/1000], Train Loss: 0.3710, Valid Loss: 0.5205\n",
            "Epoch [283/1000], Train Loss: 0.3739, Valid Loss: 0.4869\n",
            "Epoch [284/1000], Train Loss: 0.3686, Valid Loss: 0.5205\n",
            "Epoch [285/1000], Train Loss: 0.3756, Valid Loss: 0.4929\n",
            "Epoch [286/1000], Train Loss: 0.3654, Valid Loss: 0.5315\n",
            "Epoch [287/1000], Train Loss: 0.3818, Valid Loss: 0.5164\n",
            "Epoch [288/1000], Train Loss: 0.3903, Valid Loss: 0.5774\n",
            "Epoch [289/1000], Train Loss: 0.4148, Valid Loss: 0.5394\n",
            "Epoch [290/1000], Train Loss: 0.4352, Valid Loss: 0.6503\n",
            "Epoch [291/1000], Train Loss: 0.4578, Valid Loss: 0.5352\n",
            "Epoch [292/1000], Train Loss: 0.4038, Valid Loss: 0.4950\n",
            "Epoch [293/1000], Train Loss: 0.3814, Valid Loss: 0.5589\n",
            "Epoch [294/1000], Train Loss: 0.4147, Valid Loss: 0.5057\n",
            "Epoch [295/1000], Train Loss: 0.3969, Valid Loss: 0.4903\n",
            "Epoch [296/1000], Train Loss: 0.3735, Valid Loss: 0.5261\n",
            "Epoch [297/1000], Train Loss: 0.3837, Valid Loss: 0.5145\n",
            "Epoch [298/1000], Train Loss: 0.3842, Valid Loss: 0.4837\n",
            "Epoch [299/1000], Train Loss: 0.3640, Valid Loss: 0.4944\n",
            "Epoch [300/1000], Train Loss: 0.3751, Valid Loss: 0.4903\n",
            "Epoch [301/1000], Train Loss: 0.3651, Valid Loss: 0.4953\n",
            "Epoch [302/1000], Train Loss: 0.3576, Valid Loss: 0.4913\n",
            "Epoch [303/1000], Train Loss: 0.3612, Valid Loss: 0.4903\n",
            "Epoch [304/1000], Train Loss: 0.3519, Valid Loss: 0.4952\n",
            "Epoch [305/1000], Train Loss: 0.3485, Valid Loss: 0.4849\n",
            "Epoch [306/1000], Train Loss: 0.3484, Valid Loss: 0.4832\n",
            "Epoch [307/1000], Train Loss: 0.3400, Valid Loss: 0.4673\n",
            "Epoch [308/1000], Train Loss: 0.3399, Valid Loss: 0.4734\n",
            "Epoch [309/1000], Train Loss: 0.3343, Valid Loss: 0.4649\n",
            "Epoch [310/1000], Train Loss: 0.3332, Valid Loss: 0.4681\n",
            "Epoch [311/1000], Train Loss: 0.3298, Valid Loss: 0.4682\n",
            "Epoch [312/1000], Train Loss: 0.3262, Valid Loss: 0.4657\n",
            "Epoch [313/1000], Train Loss: 0.3235, Valid Loss: 0.4741\n",
            "Epoch [314/1000], Train Loss: 0.3231, Valid Loss: 0.4638\n",
            "Epoch [315/1000], Train Loss: 0.3234, Valid Loss: 0.4884\n",
            "Epoch [316/1000], Train Loss: 0.3258, Valid Loss: 0.4616\n",
            "Epoch [317/1000], Train Loss: 0.3405, Valid Loss: 0.6149\n",
            "Epoch [318/1000], Train Loss: 0.4136, Valid Loss: 0.5367\n",
            "Epoch [319/1000], Train Loss: 0.4241, Valid Loss: 0.7629\n",
            "Epoch [320/1000], Train Loss: 0.5334, Valid Loss: 0.5250\n",
            "Epoch [321/1000], Train Loss: 0.3812, Valid Loss: 0.5938\n",
            "Epoch [322/1000], Train Loss: 0.5025, Valid Loss: 0.6990\n",
            "Epoch [323/1000], Train Loss: 0.5122, Valid Loss: 0.6373\n",
            "Epoch [324/1000], Train Loss: 0.4675, Valid Loss: 0.5902\n",
            "Epoch [325/1000], Train Loss: 0.4877, Valid Loss: 0.5628\n",
            "Epoch [326/1000], Train Loss: 0.4420, Valid Loss: 0.6132\n",
            "Epoch [327/1000], Train Loss: 0.4654, Valid Loss: 0.5662\n",
            "Epoch [328/1000], Train Loss: 0.4175, Valid Loss: 0.5197\n",
            "Epoch [329/1000], Train Loss: 0.4192, Valid Loss: 0.5143\n",
            "Epoch [330/1000], Train Loss: 0.4198, Valid Loss: 0.5323\n",
            "Epoch [331/1000], Train Loss: 0.4109, Valid Loss: 0.5426\n",
            "Epoch [332/1000], Train Loss: 0.3992, Valid Loss: 0.5130\n",
            "Epoch [333/1000], Train Loss: 0.3852, Valid Loss: 0.4991\n",
            "Epoch [334/1000], Train Loss: 0.3802, Valid Loss: 0.4981\n",
            "Epoch [335/1000], Train Loss: 0.3719, Valid Loss: 0.5184\n",
            "Epoch [336/1000], Train Loss: 0.3752, Valid Loss: 0.5082\n",
            "Epoch [337/1000], Train Loss: 0.3636, Valid Loss: 0.4921\n",
            "Epoch [338/1000], Train Loss: 0.3560, Valid Loss: 0.4797\n",
            "Epoch [339/1000], Train Loss: 0.3520, Valid Loss: 0.4798\n",
            "Epoch [340/1000], Train Loss: 0.3460, Valid Loss: 0.4894\n",
            "Epoch [341/1000], Train Loss: 0.3457, Valid Loss: 0.4814\n",
            "Epoch [342/1000], Train Loss: 0.3431, Valid Loss: 0.4679\n",
            "Epoch [343/1000], Train Loss: 0.3378, Valid Loss: 0.4659\n",
            "Epoch [344/1000], Train Loss: 0.3331, Valid Loss: 0.4715\n",
            "Epoch [345/1000], Train Loss: 0.3271, Valid Loss: 0.4771\n",
            "Epoch [346/1000], Train Loss: 0.3266, Valid Loss: 0.4693\n",
            "Epoch [347/1000], Train Loss: 0.3231, Valid Loss: 0.4649\n",
            "Epoch [348/1000], Train Loss: 0.3205, Valid Loss: 0.4651\n",
            "Epoch [349/1000], Train Loss: 0.3167, Valid Loss: 0.4583\n",
            "Epoch [350/1000], Train Loss: 0.3132, Valid Loss: 0.4561\n",
            "Epoch [351/1000], Train Loss: 0.3109, Valid Loss: 0.4608\n",
            "Epoch [352/1000], Train Loss: 0.3082, Valid Loss: 0.4597\n",
            "Epoch [353/1000], Train Loss: 0.3062, Valid Loss: 0.4539\n",
            "Epoch [354/1000], Train Loss: 0.3044, Valid Loss: 0.4574\n",
            "Epoch [355/1000], Train Loss: 0.3015, Valid Loss: 0.4527\n",
            "Epoch [356/1000], Train Loss: 0.2980, Valid Loss: 0.4510\n",
            "Epoch [357/1000], Train Loss: 0.2966, Valid Loss: 0.4521\n",
            "Epoch [358/1000], Train Loss: 0.2947, Valid Loss: 0.4451\n",
            "Epoch [359/1000], Train Loss: 0.2927, Valid Loss: 0.4470\n",
            "Epoch [360/1000], Train Loss: 0.2904, Valid Loss: 0.4500\n",
            "Epoch [361/1000], Train Loss: 0.2881, Valid Loss: 0.4459\n",
            "Epoch [362/1000], Train Loss: 0.2871, Valid Loss: 0.4544\n",
            "Epoch [363/1000], Train Loss: 0.2854, Valid Loss: 0.4493\n",
            "Epoch [364/1000], Train Loss: 0.2833, Valid Loss: 0.4504\n",
            "Epoch [365/1000], Train Loss: 0.2820, Valid Loss: 0.4493\n",
            "Epoch [366/1000], Train Loss: 0.2801, Valid Loss: 0.4507\n",
            "Epoch [367/1000], Train Loss: 0.2784, Valid Loss: 0.4520\n",
            "Epoch [368/1000], Train Loss: 0.2765, Valid Loss: 0.4422\n",
            "Epoch [369/1000], Train Loss: 0.2757, Valid Loss: 0.4485\n",
            "Epoch [370/1000], Train Loss: 0.2739, Valid Loss: 0.4401\n",
            "Epoch [371/1000], Train Loss: 0.2734, Valid Loss: 0.4582\n",
            "Epoch [372/1000], Train Loss: 0.2746, Valid Loss: 0.4393\n",
            "Epoch [373/1000], Train Loss: 0.2766, Valid Loss: 0.4970\n",
            "Epoch [374/1000], Train Loss: 0.2885, Valid Loss: 0.4480\n",
            "Epoch [375/1000], Train Loss: 0.2927, Valid Loss: 0.5570\n",
            "Epoch [376/1000], Train Loss: 0.3290, Valid Loss: 0.4594\n",
            "Epoch [377/1000], Train Loss: 0.3039, Valid Loss: 0.5566\n",
            "Epoch [378/1000], Train Loss: 0.3300, Valid Loss: 0.4541\n",
            "Epoch [379/1000], Train Loss: 0.2859, Valid Loss: 0.4583\n",
            "Epoch [380/1000], Train Loss: 0.2897, Valid Loss: 0.5038\n",
            "Epoch [381/1000], Train Loss: 0.3004, Valid Loss: 0.4668\n",
            "Epoch [382/1000], Train Loss: 0.3020, Valid Loss: 0.5133\n",
            "Epoch [383/1000], Train Loss: 0.3154, Valid Loss: 0.4643\n",
            "Epoch [384/1000], Train Loss: 0.2905, Valid Loss: 0.4772\n",
            "Epoch [385/1000], Train Loss: 0.2856, Valid Loss: 0.4871\n",
            "Epoch [386/1000], Train Loss: 0.2916, Valid Loss: 0.4679\n",
            "Epoch [387/1000], Train Loss: 0.2903, Valid Loss: 0.4947\n",
            "Epoch [388/1000], Train Loss: 0.2822, Valid Loss: 0.4776\n",
            "Epoch [389/1000], Train Loss: 0.2856, Valid Loss: 0.4873\n",
            "Epoch [390/1000], Train Loss: 0.2860, Valid Loss: 0.4665\n",
            "Epoch [391/1000], Train Loss: 0.2827, Valid Loss: 0.4833\n",
            "Epoch [392/1000], Train Loss: 0.2796, Valid Loss: 0.4633\n",
            "Epoch [393/1000], Train Loss: 0.2744, Valid Loss: 0.4744\n",
            "Epoch [394/1000], Train Loss: 0.2775, Valid Loss: 0.4610\n",
            "Epoch [395/1000], Train Loss: 0.2743, Valid Loss: 0.4543\n",
            "Epoch [396/1000], Train Loss: 0.2668, Valid Loss: 0.4662\n",
            "Epoch [397/1000], Train Loss: 0.2671, Valid Loss: 0.4580\n",
            "Epoch [398/1000], Train Loss: 0.2647, Valid Loss: 0.4696\n",
            "Epoch [399/1000], Train Loss: 0.2630, Valid Loss: 0.4494\n",
            "Epoch [400/1000], Train Loss: 0.2662, Valid Loss: 0.4872\n",
            "Epoch [401/1000], Train Loss: 0.2703, Valid Loss: 0.4535\n",
            "Epoch [402/1000], Train Loss: 0.2719, Valid Loss: 0.5427\n",
            "Epoch [403/1000], Train Loss: 0.3001, Valid Loss: 0.4867\n",
            "Epoch [404/1000], Train Loss: 0.3202, Valid Loss: 0.7271\n",
            "Epoch [405/1000], Train Loss: 0.4304, Valid Loss: 0.5340\n",
            "Epoch [406/1000], Train Loss: 0.3420, Valid Loss: 0.5831\n",
            "Epoch [407/1000], Train Loss: 0.3331, Valid Loss: 0.5733\n",
            "Epoch [408/1000], Train Loss: 0.3508, Valid Loss: 0.6715\n",
            "Epoch [409/1000], Train Loss: 0.4401, Valid Loss: 0.6439\n",
            "Epoch [410/1000], Train Loss: 0.4038, Valid Loss: 0.6116\n",
            "Epoch [411/1000], Train Loss: 0.3675, Valid Loss: 0.6032\n",
            "Epoch [412/1000], Train Loss: 0.3954, Valid Loss: 0.5883\n",
            "Epoch [413/1000], Train Loss: 0.3653, Valid Loss: 0.5149\n",
            "Epoch [414/1000], Train Loss: 0.3480, Valid Loss: 0.4901\n",
            "Epoch [415/1000], Train Loss: 0.3436, Valid Loss: 0.5084\n",
            "Epoch [416/1000], Train Loss: 0.3352, Valid Loss: 0.5292\n",
            "Epoch [417/1000], Train Loss: 0.3438, Valid Loss: 0.5060\n",
            "Epoch [418/1000], Train Loss: 0.3160, Valid Loss: 0.4832\n",
            "Epoch [419/1000], Train Loss: 0.3178, Valid Loss: 0.4797\n",
            "Epoch [420/1000], Train Loss: 0.3073, Valid Loss: 0.4853\n",
            "Epoch [421/1000], Train Loss: 0.3117, Valid Loss: 0.4825\n",
            "Epoch [422/1000], Train Loss: 0.3039, Valid Loss: 0.4748\n",
            "Epoch [423/1000], Train Loss: 0.2989, Valid Loss: 0.4837\n",
            "Epoch [424/1000], Train Loss: 0.2981, Valid Loss: 0.4624\n",
            "Epoch [425/1000], Train Loss: 0.2897, Valid Loss: 0.4490\n",
            "Epoch [426/1000], Train Loss: 0.2869, Valid Loss: 0.4601\n",
            "Epoch [427/1000], Train Loss: 0.2807, Valid Loss: 0.4581\n",
            "Epoch [428/1000], Train Loss: 0.2764, Valid Loss: 0.4450\n",
            "Epoch [429/1000], Train Loss: 0.2746, Valid Loss: 0.4546\n",
            "Epoch [430/1000], Train Loss: 0.2707, Valid Loss: 0.4491\n",
            "Epoch [431/1000], Train Loss: 0.2663, Valid Loss: 0.4408\n",
            "Epoch [432/1000], Train Loss: 0.2661, Valid Loss: 0.4493\n",
            "Epoch [433/1000], Train Loss: 0.2601, Valid Loss: 0.4399\n",
            "Epoch [434/1000], Train Loss: 0.2583, Valid Loss: 0.4502\n",
            "Epoch [435/1000], Train Loss: 0.2549, Valid Loss: 0.4579\n",
            "Epoch [436/1000], Train Loss: 0.2535, Valid Loss: 0.4418\n",
            "Epoch [437/1000], Train Loss: 0.2510, Valid Loss: 0.4574\n",
            "Epoch [438/1000], Train Loss: 0.2502, Valid Loss: 0.4401\n",
            "Epoch [439/1000], Train Loss: 0.2461, Valid Loss: 0.4499\n",
            "Epoch [440/1000], Train Loss: 0.2457, Valid Loss: 0.4344\n",
            "Epoch [441/1000], Train Loss: 0.2419, Valid Loss: 0.4361\n",
            "Epoch [442/1000], Train Loss: 0.2392, Valid Loss: 0.4383\n",
            "Epoch [443/1000], Train Loss: 0.2374, Valid Loss: 0.4404\n",
            "Epoch [444/1000], Train Loss: 0.2368, Valid Loss: 0.4459\n",
            "Epoch [445/1000], Train Loss: 0.2341, Valid Loss: 0.4397\n",
            "Epoch [446/1000], Train Loss: 0.2323, Valid Loss: 0.4444\n",
            "Epoch [447/1000], Train Loss: 0.2306, Valid Loss: 0.4348\n",
            "Epoch [448/1000], Train Loss: 0.2299, Valid Loss: 0.4523\n",
            "Epoch [449/1000], Train Loss: 0.2300, Valid Loss: 0.4372\n",
            "Epoch [450/1000], Train Loss: 0.2318, Valid Loss: 0.4963\n",
            "Epoch [451/1000], Train Loss: 0.2460, Valid Loss: 0.4487\n",
            "Epoch [452/1000], Train Loss: 0.2665, Valid Loss: 0.7067\n",
            "Epoch [453/1000], Train Loss: 0.3914, Valid Loss: 0.5271\n",
            "Epoch [454/1000], Train Loss: 0.3483, Valid Loss: 0.8690\n",
            "Epoch [455/1000], Train Loss: 0.5207, Valid Loss: 0.5444\n",
            "Epoch [456/1000], Train Loss: 0.3284, Valid Loss: 0.6918\n",
            "Epoch [457/1000], Train Loss: 0.5177, Valid Loss: 0.7866\n",
            "Epoch [458/1000], Train Loss: 0.4797, Valid Loss: 0.6386\n",
            "Epoch [459/1000], Train Loss: 0.4078, Valid Loss: 0.6567\n",
            "Epoch [460/1000], Train Loss: 0.4973, Valid Loss: 0.6077\n",
            "Epoch [461/1000], Train Loss: 0.3959, Valid Loss: 0.6400\n",
            "Epoch [462/1000], Train Loss: 0.4028, Valid Loss: 0.5196\n",
            "Epoch [463/1000], Train Loss: 0.3333, Valid Loss: 0.5790\n",
            "Epoch [464/1000], Train Loss: 0.3844, Valid Loss: 0.5816\n",
            "Epoch [465/1000], Train Loss: 0.3397, Valid Loss: 0.5866\n",
            "Epoch [466/1000], Train Loss: 0.3635, Valid Loss: 0.5088\n",
            "Epoch [467/1000], Train Loss: 0.3254, Valid Loss: 0.4986\n",
            "Epoch [468/1000], Train Loss: 0.3214, Valid Loss: 0.4945\n",
            "Epoch [469/1000], Train Loss: 0.3105, Valid Loss: 0.5070\n",
            "Epoch [470/1000], Train Loss: 0.3138, Valid Loss: 0.4721\n",
            "Epoch [471/1000], Train Loss: 0.2995, Valid Loss: 0.4815\n",
            "Epoch [472/1000], Train Loss: 0.2994, Valid Loss: 0.4726\n",
            "Epoch [473/1000], Train Loss: 0.2821, Valid Loss: 0.4838\n",
            "Epoch [474/1000], Train Loss: 0.2883, Valid Loss: 0.4775\n",
            "Epoch [475/1000], Train Loss: 0.2783, Valid Loss: 0.4725\n",
            "Epoch [476/1000], Train Loss: 0.2748, Valid Loss: 0.4620\n",
            "Epoch [477/1000], Train Loss: 0.2731, Valid Loss: 0.4537\n",
            "Epoch [478/1000], Train Loss: 0.2661, Valid Loss: 0.4617\n",
            "Epoch [479/1000], Train Loss: 0.2603, Valid Loss: 0.4586\n",
            "Epoch [480/1000], Train Loss: 0.2585, Valid Loss: 0.4562\n",
            "Epoch [481/1000], Train Loss: 0.2544, Valid Loss: 0.4540\n",
            "Epoch [482/1000], Train Loss: 0.2502, Valid Loss: 0.4608\n",
            "Epoch [483/1000], Train Loss: 0.2491, Valid Loss: 0.4630\n",
            "Epoch [484/1000], Train Loss: 0.2442, Valid Loss: 0.4590\n",
            "Epoch [485/1000], Train Loss: 0.2418, Valid Loss: 0.4546\n",
            "Epoch [486/1000], Train Loss: 0.2391, Valid Loss: 0.4480\n",
            "Epoch [487/1000], Train Loss: 0.2364, Valid Loss: 0.4445\n",
            "Epoch [488/1000], Train Loss: 0.2336, Valid Loss: 0.4465\n",
            "Epoch [489/1000], Train Loss: 0.2308, Valid Loss: 0.4497\n",
            "Epoch [490/1000], Train Loss: 0.2286, Valid Loss: 0.4471\n",
            "Epoch [491/1000], Train Loss: 0.2268, Valid Loss: 0.4439\n",
            "Epoch [492/1000], Train Loss: 0.2244, Valid Loss: 0.4443\n",
            "Epoch [493/1000], Train Loss: 0.2229, Valid Loss: 0.4425\n",
            "Epoch [494/1000], Train Loss: 0.2206, Valid Loss: 0.4432\n",
            "Epoch [495/1000], Train Loss: 0.2192, Valid Loss: 0.4442\n",
            "Epoch [496/1000], Train Loss: 0.2169, Valid Loss: 0.4442\n",
            "Epoch [497/1000], Train Loss: 0.2152, Valid Loss: 0.4458\n",
            "Epoch [498/1000], Train Loss: 0.2140, Valid Loss: 0.4501\n",
            "Epoch [499/1000], Train Loss: 0.2127, Valid Loss: 0.4440\n",
            "Epoch [500/1000], Train Loss: 0.2113, Valid Loss: 0.4393\n",
            "Epoch [501/1000], Train Loss: 0.2097, Valid Loss: 0.4404\n",
            "Epoch [502/1000], Train Loss: 0.2085, Valid Loss: 0.4381\n",
            "Epoch [503/1000], Train Loss: 0.2074, Valid Loss: 0.4409\n",
            "Epoch [504/1000], Train Loss: 0.2057, Valid Loss: 0.4389\n",
            "Epoch [505/1000], Train Loss: 0.2045, Valid Loss: 0.4350\n",
            "Epoch [506/1000], Train Loss: 0.2036, Valid Loss: 0.4390\n",
            "Epoch [507/1000], Train Loss: 0.2022, Valid Loss: 0.4395\n",
            "Epoch [508/1000], Train Loss: 0.2010, Valid Loss: 0.4376\n",
            "Epoch [509/1000], Train Loss: 0.1999, Valid Loss: 0.4417\n",
            "Epoch [510/1000], Train Loss: 0.1990, Valid Loss: 0.4397\n",
            "Epoch [511/1000], Train Loss: 0.1976, Valid Loss: 0.4374\n",
            "Epoch [512/1000], Train Loss: 0.1967, Valid Loss: 0.4415\n",
            "Epoch [513/1000], Train Loss: 0.1956, Valid Loss: 0.4412\n",
            "Epoch [514/1000], Train Loss: 0.1947, Valid Loss: 0.4416\n",
            "Epoch [515/1000], Train Loss: 0.1938, Valid Loss: 0.4424\n",
            "Epoch [516/1000], Train Loss: 0.1930, Valid Loss: 0.4461\n",
            "Epoch [517/1000], Train Loss: 0.1925, Valid Loss: 0.4444\n",
            "Epoch [518/1000], Train Loss: 0.1913, Valid Loss: 0.4482\n",
            "Epoch [519/1000], Train Loss: 0.1904, Valid Loss: 0.4430\n",
            "Epoch [520/1000], Train Loss: 0.1893, Valid Loss: 0.4520\n",
            "Epoch [521/1000], Train Loss: 0.1889, Valid Loss: 0.4365\n",
            "Epoch [522/1000], Train Loss: 0.1888, Valid Loss: 0.4590\n",
            "Epoch [523/1000], Train Loss: 0.1905, Valid Loss: 0.4298\n",
            "Epoch [524/1000], Train Loss: 0.1950, Valid Loss: 0.5198\n",
            "Epoch [525/1000], Train Loss: 0.2138, Valid Loss: 0.4371\n",
            "Epoch [526/1000], Train Loss: 0.2219, Valid Loss: 0.6859\n",
            "Epoch [527/1000], Train Loss: 0.2878, Valid Loss: 0.4540\n",
            "Epoch [528/1000], Train Loss: 0.2395, Valid Loss: 0.5826\n",
            "Epoch [529/1000], Train Loss: 0.2497, Valid Loss: 0.4607\n",
            "Epoch [530/1000], Train Loss: 0.2093, Valid Loss: 0.4438\n",
            "Epoch [531/1000], Train Loss: 0.2196, Valid Loss: 0.6498\n",
            "Epoch [532/1000], Train Loss: 0.2869, Valid Loss: 0.4820\n",
            "Epoch [533/1000], Train Loss: 0.2556, Valid Loss: 0.5361\n",
            "Epoch [534/1000], Train Loss: 0.2495, Valid Loss: 0.4957\n",
            "Epoch [535/1000], Train Loss: 0.2190, Valid Loss: 0.4818\n",
            "Epoch [536/1000], Train Loss: 0.2549, Valid Loss: 0.6537\n",
            "Epoch [537/1000], Train Loss: 0.2861, Valid Loss: 0.4864\n",
            "Epoch [538/1000], Train Loss: 0.2374, Valid Loss: 0.5305\n",
            "Epoch [539/1000], Train Loss: 0.2733, Valid Loss: 0.6754\n",
            "Epoch [540/1000], Train Loss: 0.3022, Valid Loss: 0.4980\n",
            "Epoch [541/1000], Train Loss: 0.2539, Valid Loss: 0.4966\n",
            "Epoch [542/1000], Train Loss: 0.2399, Valid Loss: 0.6445\n",
            "Epoch [543/1000], Train Loss: 0.2715, Valid Loss: 0.4768\n",
            "Epoch [544/1000], Train Loss: 0.2542, Valid Loss: 0.5396\n",
            "Epoch [545/1000], Train Loss: 0.2665, Valid Loss: 0.5284\n",
            "Epoch [546/1000], Train Loss: 0.2467, Valid Loss: 0.5447\n",
            "Epoch [547/1000], Train Loss: 0.2768, Valid Loss: 0.6671\n",
            "Epoch [548/1000], Train Loss: 0.2791, Valid Loss: 0.5893\n",
            "Epoch [549/1000], Train Loss: 0.2556, Valid Loss: 0.5428\n",
            "Epoch [550/1000], Train Loss: 0.2939, Valid Loss: 0.7378\n",
            "Epoch [551/1000], Train Loss: 0.2928, Valid Loss: 0.5491\n",
            "Epoch [552/1000], Train Loss: 0.2979, Valid Loss: 0.4988\n",
            "Epoch [553/1000], Train Loss: 0.2597, Valid Loss: 0.5337\n",
            "Epoch [554/1000], Train Loss: 0.2644, Valid Loss: 0.4772\n",
            "Epoch [555/1000], Train Loss: 0.2521, Valid Loss: 0.4786\n",
            "Epoch [556/1000], Train Loss: 0.2565, Valid Loss: 0.5230\n",
            "Epoch [557/1000], Train Loss: 0.2514, Valid Loss: 0.4765\n",
            "Epoch [558/1000], Train Loss: 0.2378, Valid Loss: 0.4789\n",
            "Epoch [559/1000], Train Loss: 0.2279, Valid Loss: 0.4981\n",
            "Epoch [560/1000], Train Loss: 0.2300, Valid Loss: 0.4606\n",
            "Epoch [561/1000], Train Loss: 0.2264, Valid Loss: 0.4583\n",
            "Epoch [562/1000], Train Loss: 0.2166, Valid Loss: 0.4826\n",
            "Epoch [563/1000], Train Loss: 0.2139, Valid Loss: 0.4570\n",
            "Epoch [564/1000], Train Loss: 0.2126, Valid Loss: 0.4499\n",
            "Epoch [565/1000], Train Loss: 0.2036, Valid Loss: 0.4550\n",
            "Epoch [566/1000], Train Loss: 0.2056, Valid Loss: 0.4357\n",
            "Epoch [567/1000], Train Loss: 0.2054, Valid Loss: 0.4383\n",
            "Epoch [568/1000], Train Loss: 0.1968, Valid Loss: 0.4371\n",
            "Epoch [569/1000], Train Loss: 0.1952, Valid Loss: 0.4241\n",
            "Epoch [570/1000], Train Loss: 0.1990, Valid Loss: 0.4514\n",
            "Epoch [571/1000], Train Loss: 0.1956, Valid Loss: 0.4444\n",
            "Epoch [572/1000], Train Loss: 0.1952, Valid Loss: 0.4379\n",
            "Epoch [573/1000], Train Loss: 0.1928, Valid Loss: 0.4545\n",
            "Epoch [574/1000], Train Loss: 0.1903, Valid Loss: 0.4347\n",
            "Epoch [575/1000], Train Loss: 0.1895, Valid Loss: 0.4373\n",
            "Epoch [576/1000], Train Loss: 0.1857, Valid Loss: 0.4300\n",
            "Epoch [577/1000], Train Loss: 0.1847, Valid Loss: 0.4311\n",
            "Epoch [578/1000], Train Loss: 0.1823, Valid Loss: 0.4494\n",
            "Epoch [579/1000], Train Loss: 0.1818, Valid Loss: 0.4264\n",
            "Epoch [580/1000], Train Loss: 0.1793, Valid Loss: 0.4322\n",
            "Epoch [581/1000], Train Loss: 0.1766, Valid Loss: 0.4391\n",
            "Epoch [582/1000], Train Loss: 0.1761, Valid Loss: 0.4274\n",
            "Epoch [583/1000], Train Loss: 0.1751, Valid Loss: 0.4383\n",
            "Epoch [584/1000], Train Loss: 0.1744, Valid Loss: 0.4259\n",
            "Epoch [585/1000], Train Loss: 0.1734, Valid Loss: 0.4374\n",
            "Epoch [586/1000], Train Loss: 0.1717, Valid Loss: 0.4195\n",
            "Epoch [587/1000], Train Loss: 0.1695, Valid Loss: 0.4240\n",
            "Epoch [588/1000], Train Loss: 0.1690, Valid Loss: 0.4267\n",
            "Epoch [589/1000], Train Loss: 0.1674, Valid Loss: 0.4258\n",
            "Epoch [590/1000], Train Loss: 0.1669, Valid Loss: 0.4329\n",
            "Epoch [591/1000], Train Loss: 0.1662, Valid Loss: 0.4217\n",
            "Epoch [592/1000], Train Loss: 0.1664, Valid Loss: 0.4575\n",
            "Epoch [593/1000], Train Loss: 0.1676, Valid Loss: 0.4280\n",
            "Epoch [594/1000], Train Loss: 0.1686, Valid Loss: 0.4799\n",
            "Epoch [595/1000], Train Loss: 0.1769, Valid Loss: 0.4220\n",
            "Epoch [596/1000], Train Loss: 0.1793, Valid Loss: 0.5955\n",
            "Epoch [597/1000], Train Loss: 0.2096, Valid Loss: 0.4248\n",
            "Epoch [598/1000], Train Loss: 0.1943, Valid Loss: 0.5569\n",
            "Epoch [599/1000], Train Loss: 0.2106, Valid Loss: 0.4335\n",
            "Epoch [600/1000], Train Loss: 0.1747, Valid Loss: 0.4564\n",
            "Epoch [601/1000], Train Loss: 0.1657, Valid Loss: 0.4759\n",
            "Epoch [602/1000], Train Loss: 0.1686, Valid Loss: 0.4355\n",
            "Epoch [603/1000], Train Loss: 0.1792, Valid Loss: 0.5597\n",
            "Epoch [604/1000], Train Loss: 0.1987, Valid Loss: 0.4313\n",
            "Epoch [605/1000], Train Loss: 0.1841, Valid Loss: 0.4831\n",
            "Epoch [606/1000], Train Loss: 0.1749, Valid Loss: 0.4436\n",
            "Epoch [607/1000], Train Loss: 0.1638, Valid Loss: 0.4395\n",
            "Epoch [608/1000], Train Loss: 0.1662, Valid Loss: 0.4949\n",
            "Epoch [609/1000], Train Loss: 0.1731, Valid Loss: 0.4296\n",
            "Epoch [610/1000], Train Loss: 0.1730, Valid Loss: 0.5544\n",
            "Epoch [611/1000], Train Loss: 0.1759, Valid Loss: 0.4417\n",
            "Epoch [612/1000], Train Loss: 0.1678, Valid Loss: 0.4663\n",
            "Epoch [613/1000], Train Loss: 0.1583, Valid Loss: 0.4624\n",
            "Epoch [614/1000], Train Loss: 0.1578, Valid Loss: 0.4455\n",
            "Epoch [615/1000], Train Loss: 0.1586, Valid Loss: 0.4751\n",
            "Epoch [616/1000], Train Loss: 0.1612, Valid Loss: 0.4329\n",
            "Epoch [617/1000], Train Loss: 0.1606, Valid Loss: 0.5462\n",
            "Epoch [618/1000], Train Loss: 0.1617, Valid Loss: 0.4526\n",
            "Epoch [619/1000], Train Loss: 0.1593, Valid Loss: 0.5138\n",
            "Epoch [620/1000], Train Loss: 0.1562, Valid Loss: 0.4445\n",
            "Epoch [621/1000], Train Loss: 0.1538, Valid Loss: 0.5159\n",
            "Epoch [622/1000], Train Loss: 0.1513, Valid Loss: 0.5041\n",
            "Epoch [623/1000], Train Loss: 0.1515, Valid Loss: 0.4646\n",
            "Epoch [624/1000], Train Loss: 0.1491, Valid Loss: 0.4663\n",
            "Epoch [625/1000], Train Loss: 0.1492, Valid Loss: 0.4511\n",
            "Epoch [626/1000], Train Loss: 0.1488, Valid Loss: 0.5185\n",
            "Epoch [627/1000], Train Loss: 0.1502, Valid Loss: 0.4486\n",
            "Epoch [628/1000], Train Loss: 0.1531, Valid Loss: 0.6061\n",
            "Epoch [629/1000], Train Loss: 0.1657, Valid Loss: 0.4445\n",
            "Epoch [630/1000], Train Loss: 0.1785, Valid Loss: 0.9037\n",
            "Epoch [631/1000], Train Loss: 0.2565, Valid Loss: 0.4706\n",
            "Epoch [632/1000], Train Loss: 0.2210, Valid Loss: 0.8993\n",
            "Epoch [633/1000], Train Loss: 0.3277, Valid Loss: 0.5007\n",
            "Epoch [634/1000], Train Loss: 0.2098, Valid Loss: 0.6348\n",
            "Epoch [635/1000], Train Loss: 0.1922, Valid Loss: 0.7170\n",
            "Epoch [636/1000], Train Loss: 0.2128, Valid Loss: 0.4888\n",
            "Epoch [637/1000], Train Loss: 0.2426, Valid Loss: 0.9051\n",
            "Epoch [638/1000], Train Loss: 0.2983, Valid Loss: 0.5615\n",
            "Epoch [639/1000], Train Loss: 0.1960, Valid Loss: 0.5578\n",
            "Epoch [640/1000], Train Loss: 0.3371, Valid Loss: 2.0744\n",
            "Epoch [641/1000], Train Loss: 0.9862, Valid Loss: 0.7760\n",
            "Epoch [642/1000], Train Loss: 0.5226, Valid Loss: 1.6121\n",
            "Epoch [643/1000], Train Loss: 1.1545, Valid Loss: 2.8084\n",
            "Epoch [644/1000], Train Loss: 1.4756, Valid Loss: 1.7351\n",
            "Epoch [645/1000], Train Loss: 1.1992, Valid Loss: 1.1104\n",
            "Epoch [646/1000], Train Loss: 0.8602, Valid Loss: 1.2577\n",
            "Epoch [647/1000], Train Loss: 0.8262, Valid Loss: 1.2582\n",
            "Epoch [648/1000], Train Loss: 0.8021, Valid Loss: 0.9681\n",
            "Epoch [649/1000], Train Loss: 0.6677, Valid Loss: 0.9616\n",
            "Epoch [650/1000], Train Loss: 0.6521, Valid Loss: 0.8443\n",
            "Epoch [651/1000], Train Loss: 0.5784, Valid Loss: 0.8208\n",
            "Epoch [652/1000], Train Loss: 0.5373, Valid Loss: 0.7722\n",
            "Epoch [653/1000], Train Loss: 0.5062, Valid Loss: 0.6957\n",
            "Epoch [654/1000], Train Loss: 0.4928, Valid Loss: 0.6708\n",
            "Epoch [655/1000], Train Loss: 0.4607, Valid Loss: 0.6426\n",
            "Epoch [656/1000], Train Loss: 0.4420, Valid Loss: 0.6436\n",
            "Epoch [657/1000], Train Loss: 0.4500, Valid Loss: 0.6292\n",
            "Epoch [658/1000], Train Loss: 0.4388, Valid Loss: 0.6116\n",
            "Epoch [659/1000], Train Loss: 0.4187, Valid Loss: 0.6053\n",
            "Epoch [660/1000], Train Loss: 0.4105, Valid Loss: 0.5888\n",
            "Epoch [661/1000], Train Loss: 0.3934, Valid Loss: 0.5755\n",
            "Epoch [662/1000], Train Loss: 0.3780, Valid Loss: 0.5634\n",
            "Epoch [663/1000], Train Loss: 0.3665, Valid Loss: 0.5500\n",
            "Epoch [664/1000], Train Loss: 0.3599, Valid Loss: 0.5367\n",
            "Epoch [665/1000], Train Loss: 0.3557, Valid Loss: 0.5248\n",
            "Epoch [666/1000], Train Loss: 0.3482, Valid Loss: 0.5205\n",
            "Epoch [667/1000], Train Loss: 0.3404, Valid Loss: 0.5168\n",
            "Epoch [668/1000], Train Loss: 0.3350, Valid Loss: 0.5051\n",
            "Epoch [669/1000], Train Loss: 0.3271, Valid Loss: 0.5002\n",
            "Epoch [670/1000], Train Loss: 0.3214, Valid Loss: 0.4977\n",
            "Epoch [671/1000], Train Loss: 0.3155, Valid Loss: 0.4944\n",
            "Epoch [672/1000], Train Loss: 0.3092, Valid Loss: 0.4920\n",
            "Epoch [673/1000], Train Loss: 0.3035, Valid Loss: 0.4898\n",
            "Epoch [674/1000], Train Loss: 0.2983, Valid Loss: 0.4867\n",
            "Epoch [675/1000], Train Loss: 0.2930, Valid Loss: 0.4860\n",
            "Epoch [676/1000], Train Loss: 0.2885, Valid Loss: 0.4849\n",
            "Epoch [677/1000], Train Loss: 0.2845, Valid Loss: 0.4820\n",
            "Epoch [678/1000], Train Loss: 0.2790, Valid Loss: 0.4752\n",
            "Epoch [679/1000], Train Loss: 0.2744, Valid Loss: 0.4679\n",
            "Epoch [680/1000], Train Loss: 0.2702, Valid Loss: 0.4670\n",
            "Epoch [681/1000], Train Loss: 0.2652, Valid Loss: 0.4699\n",
            "Epoch [682/1000], Train Loss: 0.2612, Valid Loss: 0.4701\n",
            "Epoch [683/1000], Train Loss: 0.2572, Valid Loss: 0.4663\n",
            "Epoch [684/1000], Train Loss: 0.2535, Valid Loss: 0.4636\n",
            "Epoch [685/1000], Train Loss: 0.2510, Valid Loss: 0.4631\n",
            "Epoch [686/1000], Train Loss: 0.2478, Valid Loss: 0.4642\n",
            "Epoch [687/1000], Train Loss: 0.2448, Valid Loss: 0.4620\n",
            "Epoch [688/1000], Train Loss: 0.2414, Valid Loss: 0.4579\n",
            "Epoch [689/1000], Train Loss: 0.2382, Valid Loss: 0.4557\n",
            "Epoch [690/1000], Train Loss: 0.2352, Valid Loss: 0.4550\n",
            "Epoch [691/1000], Train Loss: 0.2324, Valid Loss: 0.4539\n",
            "Epoch [692/1000], Train Loss: 0.2297, Valid Loss: 0.4521\n",
            "Epoch [693/1000], Train Loss: 0.2268, Valid Loss: 0.4522\n",
            "Epoch [694/1000], Train Loss: 0.2243, Valid Loss: 0.4534\n",
            "Epoch [695/1000], Train Loss: 0.2216, Valid Loss: 0.4523\n",
            "Epoch [696/1000], Train Loss: 0.2193, Valid Loss: 0.4501\n",
            "Epoch [697/1000], Train Loss: 0.2170, Valid Loss: 0.4514\n",
            "Epoch [698/1000], Train Loss: 0.2149, Valid Loss: 0.4546\n",
            "Epoch [699/1000], Train Loss: 0.2126, Valid Loss: 0.4562\n",
            "Epoch [700/1000], Train Loss: 0.2102, Valid Loss: 0.4552\n",
            "Epoch [701/1000], Train Loss: 0.2083, Valid Loss: 0.4546\n",
            "Epoch [702/1000], Train Loss: 0.2060, Valid Loss: 0.4526\n",
            "Epoch [703/1000], Train Loss: 0.2036, Valid Loss: 0.4498\n",
            "Epoch [704/1000], Train Loss: 0.2018, Valid Loss: 0.4515\n",
            "Epoch [705/1000], Train Loss: 0.1995, Valid Loss: 0.4492\n",
            "Epoch [706/1000], Train Loss: 0.1978, Valid Loss: 0.4465\n",
            "Epoch [707/1000], Train Loss: 0.1962, Valid Loss: 0.4504\n",
            "Epoch [708/1000], Train Loss: 0.1941, Valid Loss: 0.4492\n",
            "Epoch [709/1000], Train Loss: 0.1924, Valid Loss: 0.4469\n",
            "Epoch [710/1000], Train Loss: 0.1910, Valid Loss: 0.4529\n",
            "Epoch [711/1000], Train Loss: 0.1892, Valid Loss: 0.4524\n",
            "Epoch [712/1000], Train Loss: 0.1875, Valid Loss: 0.4517\n",
            "Epoch [713/1000], Train Loss: 0.1857, Valid Loss: 0.5004\n",
            "Epoch [714/1000], Train Loss: 0.1843, Valid Loss: 0.4504\n",
            "Epoch [715/1000], Train Loss: 0.1831, Valid Loss: 0.5015\n",
            "Epoch [716/1000], Train Loss: 0.1818, Valid Loss: 0.4503\n",
            "Epoch [717/1000], Train Loss: 0.1799, Valid Loss: 0.4974\n",
            "Epoch [718/1000], Train Loss: 0.1784, Valid Loss: 0.5001\n",
            "Epoch [719/1000], Train Loss: 0.1773, Valid Loss: 0.4943\n",
            "Epoch [720/1000], Train Loss: 0.1759, Valid Loss: 0.5047\n",
            "Epoch [721/1000], Train Loss: 0.1744, Valid Loss: 0.5014\n",
            "Epoch [722/1000], Train Loss: 0.1730, Valid Loss: 0.4984\n",
            "Epoch [723/1000], Train Loss: 0.1718, Valid Loss: 0.5040\n",
            "Epoch [724/1000], Train Loss: 0.1706, Valid Loss: 0.5014\n",
            "Epoch [725/1000], Train Loss: 0.1698, Valid Loss: 0.5082\n",
            "Epoch [726/1000], Train Loss: 0.1687, Valid Loss: 0.4992\n",
            "Epoch [727/1000], Train Loss: 0.1676, Valid Loss: 0.5107\n",
            "Epoch [728/1000], Train Loss: 0.1670, Valid Loss: 0.4988\n",
            "Epoch [729/1000], Train Loss: 0.1660, Valid Loss: 0.5055\n",
            "Epoch [730/1000], Train Loss: 0.1649, Valid Loss: 0.5049\n",
            "Epoch [731/1000], Train Loss: 0.1637, Valid Loss: 0.5089\n",
            "Epoch [732/1000], Train Loss: 0.1625, Valid Loss: 0.5029\n",
            "Epoch [733/1000], Train Loss: 0.1615, Valid Loss: 0.5083\n",
            "Epoch [734/1000], Train Loss: 0.1605, Valid Loss: 0.5048\n",
            "Epoch [735/1000], Train Loss: 0.1598, Valid Loss: 0.5109\n",
            "Epoch [736/1000], Train Loss: 0.1587, Valid Loss: 0.5021\n",
            "Epoch [737/1000], Train Loss: 0.1584, Valid Loss: 0.5217\n",
            "Epoch [738/1000], Train Loss: 0.1589, Valid Loss: 0.4990\n",
            "Epoch [739/1000], Train Loss: 0.1587, Valid Loss: 0.5279\n",
            "Epoch [740/1000], Train Loss: 0.1593, Valid Loss: 0.4966\n",
            "Epoch [741/1000], Train Loss: 0.1560, Valid Loss: 0.5100\n",
            "Epoch [742/1000], Train Loss: 0.1538, Valid Loss: 0.5176\n",
            "Epoch [743/1000], Train Loss: 0.1532, Valid Loss: 0.5031\n",
            "Epoch [744/1000], Train Loss: 0.1534, Valid Loss: 0.5246\n",
            "Epoch [745/1000], Train Loss: 0.1544, Valid Loss: 0.5015\n",
            "Epoch [746/1000], Train Loss: 0.1535, Valid Loss: 0.5291\n",
            "Epoch [747/1000], Train Loss: 0.1531, Valid Loss: 0.5028\n",
            "Epoch [748/1000], Train Loss: 0.1502, Valid Loss: 0.5125\n",
            "Epoch [749/1000], Train Loss: 0.1487, Valid Loss: 0.5171\n",
            "Epoch [750/1000], Train Loss: 0.1479, Valid Loss: 0.5074\n",
            "Epoch [751/1000], Train Loss: 0.1481, Valid Loss: 0.5304\n",
            "Epoch [752/1000], Train Loss: 0.1492, Valid Loss: 0.5033\n",
            "Epoch [753/1000], Train Loss: 0.1496, Valid Loss: 0.5390\n",
            "Epoch [754/1000], Train Loss: 0.1515, Valid Loss: 0.5028\n",
            "Epoch [755/1000], Train Loss: 0.1491, Valid Loss: 0.5810\n",
            "Epoch [756/1000], Train Loss: 0.1482, Valid Loss: 0.5087\n",
            "Epoch [757/1000], Train Loss: 0.1446, Valid Loss: 0.5579\n",
            "Epoch [758/1000], Train Loss: 0.1431, Valid Loss: 0.6125\n",
            "Epoch [759/1000], Train Loss: 0.1426, Valid Loss: 0.5991\n",
            "Epoch [760/1000], Train Loss: 0.1436, Valid Loss: 0.6234\n",
            "Epoch [761/1000], Train Loss: 0.1466, Valid Loss: 0.5912\n",
            "Epoch [762/1000], Train Loss: 0.1483, Valid Loss: 0.7018\n",
            "Epoch [763/1000], Train Loss: 0.1553, Valid Loss: 0.5898\n",
            "Epoch [764/1000], Train Loss: 0.1486, Valid Loss: 0.6720\n",
            "Epoch [765/1000], Train Loss: 0.1465, Valid Loss: 0.6030\n",
            "Epoch [766/1000], Train Loss: 0.1403, Valid Loss: 0.6131\n",
            "Epoch [767/1000], Train Loss: 0.1394, Valid Loss: 0.6253\n",
            "Epoch [768/1000], Train Loss: 0.1418, Valid Loss: 0.6036\n",
            "Epoch [769/1000], Train Loss: 0.1527, Valid Loss: 0.7264\n",
            "Epoch [770/1000], Train Loss: 0.1666, Valid Loss: 0.5991\n",
            "Epoch [771/1000], Train Loss: 0.1590, Valid Loss: 0.7511\n",
            "Epoch [772/1000], Train Loss: 0.1600, Valid Loss: 0.6000\n",
            "Epoch [773/1000], Train Loss: 0.1472, Valid Loss: 0.6325\n",
            "Epoch [774/1000], Train Loss: 0.1442, Valid Loss: 0.6717\n",
            "Epoch [775/1000], Train Loss: 0.1433, Valid Loss: 0.5970\n",
            "Epoch [776/1000], Train Loss: 0.1474, Valid Loss: 0.7612\n",
            "Epoch [777/1000], Train Loss: 0.1556, Valid Loss: 0.6062\n",
            "Epoch [778/1000], Train Loss: 0.1502, Valid Loss: 0.7263\n",
            "Epoch [779/1000], Train Loss: 0.1474, Valid Loss: 0.6445\n",
            "Epoch [780/1000], Train Loss: 0.1399, Valid Loss: 0.6675\n",
            "Epoch [781/1000], Train Loss: 0.1381, Valid Loss: 0.6752\n",
            "Epoch [782/1000], Train Loss: 0.1399, Valid Loss: 0.5892\n",
            "Epoch [783/1000], Train Loss: 0.1435, Valid Loss: 0.7541\n",
            "Epoch [784/1000], Train Loss: 0.1523, Valid Loss: 0.6029\n",
            "Epoch [785/1000], Train Loss: 0.1455, Valid Loss: 0.7288\n",
            "Epoch [786/1000], Train Loss: 0.1436, Valid Loss: 0.6444\n",
            "Epoch [787/1000], Train Loss: 0.1358, Valid Loss: 0.6650\n",
            "Epoch [788/1000], Train Loss: 0.1340, Valid Loss: 0.7214\n",
            "Epoch [789/1000], Train Loss: 0.1352, Valid Loss: 0.5931\n",
            "Epoch [790/1000], Train Loss: 0.1397, Valid Loss: 0.7617\n",
            "Epoch [791/1000], Train Loss: 0.1512, Valid Loss: 0.6058\n",
            "Epoch [792/1000], Train Loss: 0.1486, Valid Loss: 0.7845\n",
            "Epoch [793/1000], Train Loss: 0.1642, Valid Loss: 0.5905\n",
            "Epoch [794/1000], Train Loss: 0.1442, Valid Loss: 0.7437\n",
            "Epoch [795/1000], Train Loss: 0.1384, Valid Loss: 0.6711\n",
            "Epoch [796/1000], Train Loss: 0.1324, Valid Loss: 0.6381\n",
            "Epoch [797/1000], Train Loss: 0.1369, Valid Loss: 0.7517\n",
            "Epoch [798/1000], Train Loss: 0.1432, Valid Loss: 0.6168\n",
            "Epoch [799/1000], Train Loss: 0.1454, Valid Loss: 0.7500\n",
            "Epoch [800/1000], Train Loss: 0.1477, Valid Loss: 0.6379\n",
            "Epoch [801/1000], Train Loss: 0.1400, Valid Loss: 0.6744\n",
            "Epoch [802/1000], Train Loss: 0.1413, Valid Loss: 0.6311\n",
            "Epoch [803/1000], Train Loss: 0.1342, Valid Loss: 0.6878\n",
            "Epoch [804/1000], Train Loss: 0.1384, Valid Loss: 0.7442\n",
            "Epoch [805/1000], Train Loss: 0.1333, Valid Loss: 0.6428\n",
            "Epoch [806/1000], Train Loss: 0.1484, Valid Loss: 0.7999\n",
            "Epoch [807/1000], Train Loss: 0.1674, Valid Loss: 0.5888\n",
            "Epoch [808/1000], Train Loss: 0.1569, Valid Loss: 0.9367\n",
            "Epoch [809/1000], Train Loss: 0.1738, Valid Loss: 0.6888\n",
            "Epoch [810/1000], Train Loss: 0.1558, Valid Loss: 0.7419\n",
            "Epoch [811/1000], Train Loss: 0.1874, Valid Loss: 0.7688\n",
            "Epoch [812/1000], Train Loss: 0.1426, Valid Loss: 0.7165\n",
            "Epoch [813/1000], Train Loss: 0.1788, Valid Loss: 0.8597\n",
            "Epoch [814/1000], Train Loss: 0.1799, Valid Loss: 0.6856\n",
            "Epoch [815/1000], Train Loss: 0.1595, Valid Loss: 0.9157\n",
            "Epoch [816/1000], Train Loss: 0.1737, Valid Loss: 0.6791\n",
            "Epoch [817/1000], Train Loss: 0.1565, Valid Loss: 0.7343\n",
            "Epoch [818/1000], Train Loss: 0.1685, Valid Loss: 0.8913\n",
            "Epoch [819/1000], Train Loss: 0.1682, Valid Loss: 0.7022\n",
            "Epoch [820/1000], Train Loss: 0.1763, Valid Loss: 0.8395\n",
            "Epoch [821/1000], Train Loss: 0.2149, Valid Loss: 0.7848\n",
            "Epoch [822/1000], Train Loss: 0.1904, Valid Loss: 0.7514\n",
            "Epoch [823/1000], Train Loss: 0.1633, Valid Loss: 0.8233\n",
            "Epoch [824/1000], Train Loss: 0.1950, Valid Loss: 0.7574\n",
            "Epoch [825/1000], Train Loss: 0.2210, Valid Loss: 0.9659\n",
            "Epoch [826/1000], Train Loss: 0.3008, Valid Loss: 0.6902\n",
            "Epoch [827/1000], Train Loss: 0.2974, Valid Loss: 1.2029\n",
            "Epoch [828/1000], Train Loss: 0.3923, Valid Loss: 1.0819\n",
            "Epoch [829/1000], Train Loss: 0.5456, Valid Loss: 1.0225\n",
            "Epoch [830/1000], Train Loss: 0.5139, Valid Loss: 0.8052\n",
            "Epoch [831/1000], Train Loss: 0.4606, Valid Loss: 1.0162\n",
            "Epoch [832/1000], Train Loss: 0.4622, Valid Loss: 0.8135\n",
            "Epoch [833/1000], Train Loss: 0.4078, Valid Loss: 1.1603\n",
            "Epoch [834/1000], Train Loss: 0.4368, Valid Loss: 0.7384\n",
            "Epoch [835/1000], Train Loss: 0.3979, Valid Loss: 0.7780\n",
            "Epoch [836/1000], Train Loss: 0.2941, Valid Loss: 0.7618\n",
            "Epoch [837/1000], Train Loss: 0.2422, Valid Loss: 0.8625\n",
            "Epoch [838/1000], Train Loss: 0.2716, Valid Loss: 0.7416\n",
            "Epoch [839/1000], Train Loss: 0.2189, Valid Loss: 0.6105\n",
            "Epoch [840/1000], Train Loss: 0.2570, Valid Loss: 0.7387\n",
            "Epoch [841/1000], Train Loss: 0.2361, Valid Loss: 0.6432\n",
            "Epoch [842/1000], Train Loss: 0.2386, Valid Loss: 0.6671\n",
            "Epoch [843/1000], Train Loss: 0.2214, Valid Loss: 0.6243\n",
            "Epoch [844/1000], Train Loss: 0.2039, Valid Loss: 0.5456\n",
            "Epoch [845/1000], Train Loss: 0.1933, Valid Loss: 0.5739\n",
            "Epoch [846/1000], Train Loss: 0.2008, Valid Loss: 0.5385\n",
            "Epoch [847/1000], Train Loss: 0.1932, Valid Loss: 0.5591\n",
            "Epoch [848/1000], Train Loss: 0.1811, Valid Loss: 0.5734\n",
            "Epoch [849/1000], Train Loss: 0.1729, Valid Loss: 0.5457\n",
            "Epoch [850/1000], Train Loss: 0.1724, Valid Loss: 0.5536\n",
            "Epoch [851/1000], Train Loss: 0.1675, Valid Loss: 0.5347\n",
            "Epoch [852/1000], Train Loss: 0.1603, Valid Loss: 0.5218\n",
            "Epoch [853/1000], Train Loss: 0.1584, Valid Loss: 0.5469\n",
            "Epoch [854/1000], Train Loss: 0.1559, Valid Loss: 0.5373\n",
            "Epoch [855/1000], Train Loss: 0.1539, Valid Loss: 0.5385\n",
            "Epoch [856/1000], Train Loss: 0.1491, Valid Loss: 0.5220\n",
            "Epoch [857/1000], Train Loss: 0.1463, Valid Loss: 0.5078\n",
            "Epoch [858/1000], Train Loss: 0.1431, Valid Loss: 0.5263\n",
            "Epoch [859/1000], Train Loss: 0.1421, Valid Loss: 0.5170\n",
            "Epoch [860/1000], Train Loss: 0.1410, Valid Loss: 0.5227\n",
            "Epoch [861/1000], Train Loss: 0.1381, Valid Loss: 0.5203\n",
            "Epoch [862/1000], Train Loss: 0.1356, Valid Loss: 0.5129\n",
            "Epoch [863/1000], Train Loss: 0.1342, Valid Loss: 0.5272\n",
            "Epoch [864/1000], Train Loss: 0.1323, Valid Loss: 0.5142\n",
            "Epoch [865/1000], Train Loss: 0.1306, Valid Loss: 0.5248\n",
            "Epoch [866/1000], Train Loss: 0.1294, Valid Loss: 0.5099\n",
            "Epoch [867/1000], Train Loss: 0.1280, Valid Loss: 0.5267\n",
            "Epoch [868/1000], Train Loss: 0.1267, Valid Loss: 0.5251\n",
            "Epoch [869/1000], Train Loss: 0.1257, Valid Loss: 0.5666\n",
            "Epoch [870/1000], Train Loss: 0.1244, Valid Loss: 0.5600\n",
            "Epoch [871/1000], Train Loss: 0.1236, Valid Loss: 0.5147\n",
            "Epoch [872/1000], Train Loss: 0.1226, Valid Loss: 0.5233\n",
            "Epoch [873/1000], Train Loss: 0.1219, Valid Loss: 0.5106\n",
            "Epoch [874/1000], Train Loss: 0.1211, Valid Loss: 0.5196\n",
            "Epoch [875/1000], Train Loss: 0.1205, Valid Loss: 0.5036\n",
            "Epoch [876/1000], Train Loss: 0.1194, Valid Loss: 0.5701\n",
            "Epoch [877/1000], Train Loss: 0.1191, Valid Loss: 0.5474\n",
            "Epoch [878/1000], Train Loss: 0.1187, Valid Loss: 0.5713\n",
            "Epoch [879/1000], Train Loss: 0.1188, Valid Loss: 0.5008\n",
            "Epoch [880/1000], Train Loss: 0.1186, Valid Loss: 0.5839\n",
            "Epoch [881/1000], Train Loss: 0.1194, Valid Loss: 0.5029\n",
            "Epoch [882/1000], Train Loss: 0.1196, Valid Loss: 0.6020\n",
            "Epoch [883/1000], Train Loss: 0.1244, Valid Loss: 0.4963\n",
            "Epoch [884/1000], Train Loss: 0.1243, Valid Loss: 0.6744\n",
            "Epoch [885/1000], Train Loss: 0.1347, Valid Loss: 0.5006\n",
            "Epoch [886/1000], Train Loss: 0.1267, Valid Loss: 0.7155\n",
            "Epoch [887/1000], Train Loss: 0.1319, Valid Loss: 0.5033\n",
            "Epoch [888/1000], Train Loss: 0.1192, Valid Loss: 0.6778\n",
            "Epoch [889/1000], Train Loss: 0.1151, Valid Loss: 0.5595\n",
            "Epoch [890/1000], Train Loss: 0.1113, Valid Loss: 0.6415\n",
            "Epoch [891/1000], Train Loss: 0.1107, Valid Loss: 0.6657\n",
            "Epoch [892/1000], Train Loss: 0.1112, Valid Loss: 0.6025\n",
            "Epoch [893/1000], Train Loss: 0.1138, Valid Loss: 0.6998\n",
            "Epoch [894/1000], Train Loss: 0.1200, Valid Loss: 0.5907\n",
            "Epoch [895/1000], Train Loss: 0.1233, Valid Loss: 0.7737\n",
            "Epoch [896/1000], Train Loss: 0.1513, Valid Loss: 0.5144\n",
            "Epoch [897/1000], Train Loss: 0.1365, Valid Loss: 0.7504\n",
            "Epoch [898/1000], Train Loss: 0.1549, Valid Loss: 0.6445\n",
            "Epoch [899/1000], Train Loss: 0.1237, Valid Loss: 0.6982\n",
            "Epoch [900/1000], Train Loss: 0.1396, Valid Loss: 0.6640\n",
            "Epoch [901/1000], Train Loss: 0.1239, Valid Loss: 0.6852\n",
            "Epoch [902/1000], Train Loss: 0.1388, Valid Loss: 0.8886\n",
            "Epoch [903/1000], Train Loss: 0.1769, Valid Loss: 0.6121\n",
            "Epoch [904/1000], Train Loss: 0.1653, Valid Loss: 0.7994\n",
            "Epoch [905/1000], Train Loss: 0.1803, Valid Loss: 0.6690\n",
            "Epoch [906/1000], Train Loss: 0.1582, Valid Loss: 0.7693\n",
            "Epoch [907/1000], Train Loss: 0.1869, Valid Loss: 0.7152\n",
            "Epoch [908/1000], Train Loss: 0.1650, Valid Loss: 0.6637\n",
            "Epoch [909/1000], Train Loss: 0.1520, Valid Loss: 0.7627\n",
            "Epoch [910/1000], Train Loss: 0.1490, Valid Loss: 0.7839\n",
            "Epoch [911/1000], Train Loss: 0.1595, Valid Loss: 0.6853\n",
            "Epoch [912/1000], Train Loss: 0.1342, Valid Loss: 0.7238\n",
            "Epoch [913/1000], Train Loss: 0.1513, Valid Loss: 0.6659\n",
            "Epoch [914/1000], Train Loss: 0.1418, Valid Loss: 0.7193\n",
            "Epoch [915/1000], Train Loss: 0.1499, Valid Loss: 0.6502\n",
            "Epoch [916/1000], Train Loss: 0.1319, Valid Loss: 0.6615\n",
            "Epoch [917/1000], Train Loss: 0.1398, Valid Loss: 0.7399\n",
            "Epoch [918/1000], Train Loss: 0.1218, Valid Loss: 0.7330\n",
            "Epoch [919/1000], Train Loss: 0.1297, Valid Loss: 0.7118\n",
            "Epoch [920/1000], Train Loss: 0.1257, Valid Loss: 0.6469\n",
            "Epoch [921/1000], Train Loss: 0.1201, Valid Loss: 0.6706\n",
            "Epoch [922/1000], Train Loss: 0.1210, Valid Loss: 0.6719\n",
            "Epoch [923/1000], Train Loss: 0.1176, Valid Loss: 0.6411\n",
            "Epoch [924/1000], Train Loss: 0.1167, Valid Loss: 0.6646\n",
            "Epoch [925/1000], Train Loss: 0.1160, Valid Loss: 0.6536\n",
            "Epoch [926/1000], Train Loss: 0.1159, Valid Loss: 0.6713\n",
            "Epoch [927/1000], Train Loss: 0.1142, Valid Loss: 0.6457\n",
            "Epoch [928/1000], Train Loss: 0.1136, Valid Loss: 0.6686\n",
            "Epoch [929/1000], Train Loss: 0.1114, Valid Loss: 0.6463\n",
            "Epoch [930/1000], Train Loss: 0.1107, Valid Loss: 0.6717\n",
            "Epoch [931/1000], Train Loss: 0.1093, Valid Loss: 0.6495\n",
            "Epoch [932/1000], Train Loss: 0.1092, Valid Loss: 0.6856\n",
            "Epoch [933/1000], Train Loss: 0.1095, Valid Loss: 0.6399\n",
            "Epoch [934/1000], Train Loss: 0.1097, Valid Loss: 0.6868\n",
            "Epoch [935/1000], Train Loss: 0.1141, Valid Loss: 0.6293\n",
            "Epoch [936/1000], Train Loss: 0.1141, Valid Loss: 0.8246\n",
            "Epoch [937/1000], Train Loss: 0.1257, Valid Loss: 0.6516\n",
            "Epoch [938/1000], Train Loss: 0.1182, Valid Loss: 0.8602\n",
            "Epoch [939/1000], Train Loss: 0.1299, Valid Loss: 0.6864\n",
            "Epoch [940/1000], Train Loss: 0.1145, Valid Loss: 0.8607\n",
            "Epoch [941/1000], Train Loss: 0.1191, Valid Loss: 0.7698\n",
            "Epoch [942/1000], Train Loss: 0.1123, Valid Loss: 0.6900\n",
            "Epoch [943/1000], Train Loss: 0.1093, Valid Loss: 0.6994\n",
            "Epoch [944/1000], Train Loss: 0.1052, Valid Loss: 0.6760\n",
            "Epoch [945/1000], Train Loss: 0.1082, Valid Loss: 0.7974\n",
            "Epoch [946/1000], Train Loss: 0.1074, Valid Loss: 0.6822\n",
            "Epoch [947/1000], Train Loss: 0.1062, Valid Loss: 0.7296\n",
            "Epoch [948/1000], Train Loss: 0.1088, Valid Loss: 0.6493\n",
            "Epoch [949/1000], Train Loss: 0.1064, Valid Loss: 0.8426\n",
            "Epoch [950/1000], Train Loss: 0.1079, Valid Loss: 0.6924\n",
            "Epoch [951/1000], Train Loss: 0.1038, Valid Loss: 0.8246\n",
            "Epoch [952/1000], Train Loss: 0.1014, Valid Loss: 0.7540\n",
            "Epoch [953/1000], Train Loss: 0.0998, Valid Loss: 0.7702\n",
            "Epoch [954/1000], Train Loss: 0.0976, Valid Loss: 0.6655\n",
            "Epoch [955/1000], Train Loss: 0.0985, Valid Loss: 0.7879\n",
            "Epoch [956/1000], Train Loss: 0.0976, Valid Loss: 0.7181\n",
            "Epoch [957/1000], Train Loss: 0.0979, Valid Loss: 0.7772\n",
            "Epoch [958/1000], Train Loss: 0.1000, Valid Loss: 0.6940\n",
            "Epoch [959/1000], Train Loss: 0.1013, Valid Loss: 0.9647\n",
            "Epoch [960/1000], Train Loss: 0.1127, Valid Loss: 0.7108\n",
            "Epoch [961/1000], Train Loss: 0.1137, Valid Loss: 0.9508\n",
            "Epoch [962/1000], Train Loss: 0.1452, Valid Loss: 0.6895\n",
            "Epoch [963/1000], Train Loss: 0.1319, Valid Loss: 1.2567\n",
            "Epoch [964/1000], Train Loss: 0.1987, Valid Loss: 0.7339\n",
            "Epoch [965/1000], Train Loss: 0.1810, Valid Loss: 1.0319\n",
            "Epoch [966/1000], Train Loss: 0.2889, Valid Loss: 0.8579\n",
            "Epoch [967/1000], Train Loss: 0.2177, Valid Loss: 1.2078\n",
            "Epoch [968/1000], Train Loss: 0.2976, Valid Loss: 0.9470\n",
            "Epoch [969/1000], Train Loss: 0.2244, Valid Loss: 0.9742\n",
            "Epoch [970/1000], Train Loss: 0.2766, Valid Loss: 1.6032\n",
            "Epoch [971/1000], Train Loss: 0.5200, Valid Loss: 1.2093\n",
            "Epoch [972/1000], Train Loss: 0.6160, Valid Loss: 1.8228\n",
            "Epoch [973/1000], Train Loss: 0.6496, Valid Loss: 1.0794\n",
            "Epoch [974/1000], Train Loss: 0.4670, Valid Loss: 1.0687\n",
            "Epoch [975/1000], Train Loss: 0.6658, Valid Loss: 1.4803\n",
            "Epoch [976/1000], Train Loss: 0.6259, Valid Loss: 1.2722\n",
            "Epoch [977/1000], Train Loss: 0.6157, Valid Loss: 1.1722\n",
            "Epoch [978/1000], Train Loss: 0.7080, Valid Loss: 1.1412\n",
            "Epoch [979/1000], Train Loss: 0.4508, Valid Loss: 1.3126\n",
            "Epoch [980/1000], Train Loss: 0.5060, Valid Loss: 0.9291\n",
            "Epoch [981/1000], Train Loss: 0.4786, Valid Loss: 1.0812\n",
            "Epoch [982/1000], Train Loss: 0.5058, Valid Loss: 1.0098\n",
            "Epoch [983/1000], Train Loss: 0.4245, Valid Loss: 0.8326\n",
            "Epoch [984/1000], Train Loss: 0.3903, Valid Loss: 0.7714\n",
            "Epoch [985/1000], Train Loss: 0.3913, Valid Loss: 0.6983\n",
            "Epoch [986/1000], Train Loss: 0.3265, Valid Loss: 0.7944\n",
            "Epoch [987/1000], Train Loss: 0.3456, Valid Loss: 0.6657\n",
            "Epoch [988/1000], Train Loss: 0.2893, Valid Loss: 0.6537\n",
            "Epoch [989/1000], Train Loss: 0.3253, Valid Loss: 0.6607\n",
            "Epoch [990/1000], Train Loss: 0.2940, Valid Loss: 0.6342\n",
            "Epoch [991/1000], Train Loss: 0.2665, Valid Loss: 0.6113\n",
            "Epoch [992/1000], Train Loss: 0.2640, Valid Loss: 0.6034\n",
            "Epoch [993/1000], Train Loss: 0.2532, Valid Loss: 0.5976\n",
            "Epoch [994/1000], Train Loss: 0.2344, Valid Loss: 0.5780\n",
            "Epoch [995/1000], Train Loss: 0.2251, Valid Loss: 0.5533\n",
            "Epoch [996/1000], Train Loss: 0.2156, Valid Loss: 0.5467\n",
            "Epoch [997/1000], Train Loss: 0.2119, Valid Loss: 0.5496\n",
            "Epoch [998/1000], Train Loss: 0.2014, Valid Loss: 0.5042\n",
            "Epoch [999/1000], Train Loss: 0.1949, Valid Loss: 0.4903\n",
            "Epoch [1000/1000], Train Loss: 0.1950, Valid Loss: 0.4755\n",
            "best model at epoch 586\n",
            "Accuracy on test set: 0.8484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Подсчет метрик**"
      ],
      "metadata": {
        "id": "trv1O_rgkHcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = final_model(X_test_tensor)\n",
        "    predicted = (outputs > 0.5).float()\n",
        "\n",
        "    predicted = predicted.cpu().numpy().flatten()\n",
        "    y_test_numpy = y_test_tensor.cpu().numpy().flatten()\n",
        "\n",
        "    print(classification_report(y_test_numpy, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaoNBnMH-vsF",
        "outputId": "eacfed6a-c487-42c1-a7da-02314dea9641"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.76      0.84      1004\n",
            "         1.0       0.79      0.94      0.86       942\n",
            "\n",
            "    accuracy                           0.85      1946\n",
            "   macro avg       0.86      0.85      0.85      1946\n",
            "weighted avg       0.86      0.85      0.85      1946\n",
            "\n"
          ]
        }
      ]
    }
  ]
}